DB Experience
=============
- As a developer, I worked with all kinds of features, except managing the databases
- In terms of features, I worked with,
	- Data Modelling, design and creating Entity-Relationship diagrams
	- creating tables and establishing relations
	- creating db triggers, db views, stored procedures, user-defined functions, etc
	- Wrote complex queries involving multiple joins, subqueries, and window functions.
	- Experience with performance tuning and query optimization, including analyzing and improving execution plans.
	- Worked with various SQL dialects including PostgreSQL, MySQL, Oracle, and SQL Server.
	- Experience with handling large datasets and writing efficient queries for big data environments like Hive and Presto.

Data Sources
	- both internal and external to the organization
	- worked wih REST API, both paginated and streaming API
		- as part of connecting with them, worked on Authentication mechanisms like Oauth, JWT, etc.
	- Also, connected with internal database; and graphQL too.
	- Also, in terms of file formats, worked with structured (like csv, tsv, parquet, json, ..), unstructured and semi-structured files.

- Modules worked with databases
	- For ORM - Sqlalchemy, and for django - using django ORM
	- Modules
		- pymysql for working with MySQL db
		- pyodbc for working with MS-SQL db
		- cx_oracle for working with Oracle db
		- psycopg2 for working with postgres
		- pymongo for working with MongoDB

- ORM
	- means Object Relational Mapper
	- ORMS will help in performing db queries and manipulations using Object Oriented way.
	- Means writing queries in programming language, like python, instead of in raw SQL.
	- Advantages
		- Less code, so more productivity and easier maintenance.
		- Database type agnostic, means later we can change db from, say, mysql to MS-SQL, and
		  the written ORM will work.
		- Sanitization of queries, mainly, helps in avoiding security issues like SQL injection attacks.

	- Disadvantages
		- It involves some learnings, for each ORM.
		- ORMs will be difficult to read and maintain when we have, say, like 5 or more tables in JOIN
		- Also, for large queries, it is be slower too.

- SQL (Structured Query Language)
	- It is a programming language used for managing and manipulating relational databases.
	- It allows you to perform tasks such as creating, querying, updating, and deleting data in a database.
	- SQL statements are written in a specific syntax, consisting of keywords, clauses, and expressions.


- SQL Injection Attack
	- Its a type of vulnerability where the attacker inserts a malicious SQL query, via web form or url parameter
	- example is to add extra condition to existing queries, to succeed, like
		`' OR 1=1 --`
	  Means
			select * from users WHERE username = '$username' and password = '$password'
	  will become
			select * from users WHERE username = '$username' and password = '$password' OR 1=1 --';


- SQL solutions - https://javarevisited.blogspot.com/2017/02/top-6-sql-query-interview-questions-and-answers.html#axzz7f7DekILS
- https://www.interviewbit.com/sql-interview-questions

- Relational Databases
	- row oriented
		- Eg: Mysql, postgreSQL
		- Organizes data records row by row
		- these are write optimized
		- suitable for OLTP (online transactional processing) style applications
		  as they can manage writes to the database well.
		- Fast at retrieving a row or a set of rows but when performing an aggregation
		  it brings extra data (columns) into memory which is slower than only selecting
		  the columns that you are performing the aggregation on.
		- And the number of disks the row oriented database might need to access
		  is usually larger.
	- column oriented / columnar or C-store databases
		- Eg: RedShift, BigQuery, snowflake
		- Organizes data by field, keeping all of the data associated with a field
		  next to each other
		- these are read-optimized
		- suitable for OLAP (online Analytics Processing) style applications
		  as they support adhoc querying of the data

- ACID
	- Atomicity
		- There should not be any partially completed transaction;
		- Either it should be completely applied or rolled back.

	- Consistency
		- characteristic that requires data updated via transactions to respect the other
		  constraints or rules within the database systems to keep data in a consistent state

	- Isolation
		- Modification of one transactions, should not effect other transactions.
		- parallel transactions are in reality isolated and seem to be performed sequentially.

	- Durability
		- persistence of committed transactions.
		- Transactions and database modifications are not kept in volatile memory but
		  are saved to permanent storage, such as disks.
		- This prevents data loss during system failure, such as a power outage.

- SQL Commands
	1) DDL (Data Definition Language) Commands – to define and modify the structure of a database.
		CREATE	: To create a new database, table, view, index, or other database objects.
		ALTER 	: To modify structure of an existing database object, such as a table or a column.
		DROP 	: To delete a database object, such as a table or an index.
		TRUNCATE: To removes all records from tables.
		RENAME 	: To Rename the database objects.

	2) DCL (Data Control Language) Commands – to control user/group access to the data in database
		GRANT	: To provide privileges to a user or a role.
		REVOKE	: To revoke privileges that were previously granted

	3) DQL (Data Query Language) Commands – to perform queries on data in database to retrieve necessary information from it.
		SELECT 	: To retrive data from one or more tables.
		FROM 	: Specifies the table(s) from which to retrieve the data.
		WHERE 	: Applies conditions to filter the data
		GROUP BY: Groups the result set by one or more columns.
		HAVING 	: Applies conditions to filter the data
		ORDER BY: sorts the result set based on one or more columns.
		JOIN 	: Combines rows from two or more tables based on related columns

	4) DML (Data Manipulation Language) Commands – to access, manipulate, and modify data in a database.
		INSERT 		: It is used to insert data into a table.
		UPDATE		: It is used to update existing data within a table.
		DELETE 		: It is used to delete records from a database table.

	5) TCL (Transaction Control Language) Commands – to control transactions in a database.
		COMMIT				: Commits a transaction, making all the changes made within the transaction permanent.
		ROLLBACK			: Rolls back a transaction, undoing all the changes made within the transaction.
		SAVEPOINT			: Sets a savepoint within a transaction, allowing to roll back to that savepoint later.
		RELEASE SAVEPOINT	: Releases a savepoint within a transaction, making it no longer necessary to roll back to that savepoint.

		SET TRANSACTION		: Sets characteristics for the current transaction, such as isolation level or transaction access mode.
		BEGIN TRANSACTION (or BEGIN WORK)	: Starts a new transaction.
		SET TRANSACTION READ ONLY			: Sets the transaction to read-only mode, indicating that no modifications will be made within the transaction.
		SET TRANSACTION READ WRITE			: Sets the transaction to read-write mode, allowing modifications to be made within the transaction.

	6) Others
		LOCK		: Table control concurrency.
		CALL		: Call a PL/SQL or JAVA subprogram.
		EXPLAIN PLAN: It describes the access path to data.


- Order in Commands definition
	- Select, From, JOIN(ON), Where, Group BY, HAVING, Order BY, LIMIT

- order of execution sql queries/ SQL Query Order of Execution
	1) FROM clause
	2) WHERE
	3) GROUP BY
	4) HAVING
	5) SELECT
	6) DISTINCT
	7) ORDER BY
	8) LIMIT/OFFSET

- SQL constraints
	NOT NULL 	 # Ensures a column cannot have a NULL value
	UNIQUE 		 # Ensures all values in a column are unique
	PRIMARY KEY  # Identifies a record in a table, is NOT NULL & UNIQUE
	FOREIGN KEY  # References a unique record from another table
	CHECK		 # Ensures all column values satisfy a condition
	DEFAULT		 # Set a default value for a column if none is entered
	INDEX		 # Quick way of retrieving records from database

- keys
	- Primary key:
		- used to uniquely identify each row in a table and does not allow null values.
		- composite primary key
			- combination of multiple columns, to make the primary key
	- Foreign key:
		- it is one or more columns whose values are based on the primary key values from another table.
	- Unique Key:
		- Unique key identifies a each row in the table uniquely.
		- Unique key allows null values.
		- composite unique key is a unique key made up of a combination of columns

- SQL Data Types
	1) Numeric Types:
		INTEGER			: Whole numbers ranging from -2,147,483,648 to 2,147,483,647.
		SMALLINT		: Small whole numbers ranging from -32,768 to 32,767.
		BIGINT			: Large whole numbers ranging from -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807.
		DECIMAL(p, s)	: Exact numeric values with a specified precision (p) and scale (s).
		NUMERIC(p, s)	: Synonym for DECIMAL.
		FLOAT(p)		: Floating-point numbers with a variable precision, where p represents the minimum number of significant digits.
		REAL			: Single-precision floating-point numbers.
		DOUBLE PRECISION: Double-precision floating-point numbers.

	2) Character Types:
		CHAR(n)			: Fixed-length character strings of length n.
		VARCHAR(n)		: Variable-length character strings with a maximum length of n.
		TEXT			: Variable-length character strings with no specified length limit.

	3) Date and Time Types:
		DATE			: Dates in the format YYYY-MM-DD.
		TIME			: Time values in the format HH:MI:SS.
		TIMESTAMP		: Date and time values in the format YYYY-MM-DD HH:MI:SS.
		INTERVAL		: Intervals representing a period of time.

	4) Boolean Type:
		BOOLEAN			: Represents a true or false value.

	5) Binary Types:
		BINARY(n)		: Fixed-length binary strings of length n.
		VARBINARY(n)	: Variable-length binary strings with a maximum length of n.
		BLOB			: Binary large objects for storing large amounts of binary data.

	6) Enumeration Type:
		ENUM('value1', 'value2', ...): A user-defined list of values where only one value can be chosen.

	7) Set Type:
		SET('value1', 'value2', ...): A user-defined list of values where multiple values can be chosen.

	8) JSON Type:
		JSON			: Stores JSON (JavaScript Object Notation) data.

	9) Array Types:
		ARRAY			: Represents an array or a list of elements.

	10) XML Type:
		XML				: Stores XML (eXtensible Markup Language) data.

	11) Geometric Types:
		POINT			: Represents a point in a two-dimensional space.
		LINE			: Represents a straight line segment.
		LSEG			: Represents a line segment with a starting and ending point.
		BOX				: Represents a rectangular box.
		PATH			: Represents a geometric path.
		POLYGON			: Represents a closed geometric polygon.
		CIRCLE			: Represents a circle.

	12)Network Address Types:
		INET			: Stores an IPv4 or IPv6 network address.
		CIDR			: Represents an IPv4 or IPv6 network address and its associated subnet mask.
		MACADDR			: Stores a media access control (MAC) address.
		MACADDR8		: Stores an extended MAC address.

	13) Bit String Types:
		BIT(n): Fixed-length bit strings of length n.
		BIT VARYING(n): Variable-length bit strings with a maximum length of n.


- SQL Operators
	1) Logical Operators
		AND		: Returns true if both conditions are true.
		OR		: Returns true if either condition is true.
		NOT		: Negates the following condition.

	2) Comparison Operators
		= (Equal to)				: Tests if two values are equal.
		<> or != (Not equal to)		: Tests if two values are not equal.
		(Greater than)				: Tests if the left operand is greater than the right operand.
		< (Less than)				: Tests if the left operand is less than the right operand.
		= (Greater than or equal to): Tests if the left operand is greater than or equal to the right operand.
		<= (Less than or equal to)	: Tests if the left operand is less than or equal to the right operand.
		BETWEEN						: Tests if a value is within a specified range.
		LIKE						: Tests if a value matches a specified pattern.
		IN							: Tests if a value is included in a specified set of values.
		IS NULL						: Tests if a value is null.
		IS NOT NULL					: Tests if a value is not null.

	3) Arithmetic Operators
		+ (Addition)				: Adds two values.
		- (Subtraction)				: Subtracts the right operand from the left operand.
		* (Multiplication)			: Multiplies two values.
		/ (Division)				: Divides the left operand by the right operand.
		% (Modulo)					: Returns the remainder of division between left operand and right operand.

	4) String Operators:
		|| (Concatenation)			: Concatenates two or more strings.

	5) Aggregate Functions:
		COUNT()						: Returns the number of rows or non-null values.
		SUM()						: Calculates the sum of a column's values.
		AVG()						: Calculates the average of a column's values.
		MIN()						: Returns the minimum value from a column.
		MAX()						: Returns the maximum value from a column.

	6) Set Operators:
		UNION			: Combines the result sets of two or more SELECT statements.
		INTERSECT		: Returns the common rows between two or more SELECT statements.
		EXCEPT or MINUS	: Returns rows that exist in first SELECT statement but not in subsequent SELECT statement(s).

	7) Conditional Operators:
		CASE			: Performs conditional operations in SQL queries.
		COALESCE		: Returns the first non-null value in a list.
		NULLIF			: Compares two expressions and returns null if they are equal.


- Window Functions:
	- window functions operate on a set of rows and return a single aggregated value for each row.
	- Unlike regular aggregate functions, window functions retuns single aggregated value for each row.

	- Ranking Windows Functions
		1) RANK()
			- Assigns a rank to each row within a partition or result set.
			Constraints: The ORDER BY clause is required to determine the ranking order.
		2) DENSE_RANK()
			- Assigns a rank to each row within a partition or result set, with no gaps in the ranking values.
			Constraints: The ORDER BY clause is required to determine the ranking order.
		3) NTILE()
			- Divides a result set into a specified number of groups and assigns a group number to each row.
			Constraints: The number of groups must be specified.

	- Analytical window functions
		1) ROW_NUMBER()
			- generates incremental row number dynamically, for each row, based on given order
				Ex: ROW_NUMBER() OVER (ORDER BY marks) as row_num
			- If partition by clause is added, the sequencing will be for each of the partition
				Ex: ROW_NUMBER() OVER (PARTITION BY dept_name ORDER BY marks) as row_num
		2) PERCENT_RANK()
			- Calculates the relative rank of a row within a partition as a value between 0 and 1.
		3) CUME_DIST()
			- Calculates the cumulative distribution of values within a partition.

	- Value Access Window Function
		1) LEAD()
			- Accesses the value of a subsequent row within the same result set.
			- to get next record. If you give number, say 5, it will give next 5th record
			Constraints: Requires the use of an ORDER BY clause to define the order of rows.
		2) LAG()
			- Accesses the value of a previous row within the same result set.
			- to get previous record. If you give number, say 5, it will give previous 5th record
			Constraints: Requires the use of an ORDER BY clause to define the order of rows.
		3) FIRST_VALUE()
			- Returns the first value in a sorted group.
			Constraints: Requires the use of an ORDER BY clause to define the order of rows.
		4) LAST_VALUE()
			- Returns the last value in a sorted group.
			Constraints: Requires the use of an ORDER BY clause to define the order of rows.

	- Navigation Window Functions:
		a) FIRST_VALUE():
			- Returns the first value in a sorted group.
			Constraints: Requires the use of an ORDER BY clause to define the order of rows.
		b) LAST_VALUE():
			- Returns the last value in a sorted group.
			Constraints: Requires the use of an ORDER BY clause to define the order of rows.
		c) NTH_VALUE():
			- Returns the value of an expression from the nth row in an ordered set of rows.
			Constraints: Requires the use of an ORDER BY clause to define the order of rows.

	- Aggregate Functions:
		- Windows aggregate functions are combination of these regular aggregate functions with OVER clause
		1) COUNT()
			- Returns the number of rows or non-null values in a column or expression.
			Constraints: Can be used with all data types.
		2) SUM()
			- Calculates the sum of values in a column or expression.
			Constraints: Typically used with numeric data types.
		3) AVG()
			- Calculates the average of values in a column or expression.
			Constraints: Typically used with numeric data types.
		4) MIN()
			- Returns the minimum value from a column or expression.
			Constraints: Can be used with all data types.
		5) MAX()
			- Returns the maximum value from a column or expression.
			Constraints: Can be used with all data types.
		6) GROUP_CONCAT()
			- Concatenates strings from multiple rows into a single string.
			Constraints: Often used with string data types.
		7) STDDEV()
			- Calculates the standard deviation of values in a column or expression.
			Constraints: Typically used with numeric data types.
		8) VARIANCE()
			- Calculates the variance of values in a column or expression.
			Constraints: Typically used with numeric data types.

	- Statistical Window Functions:
		1) CORR()
			- Calculates the correlation coefficient between two sets of values.
		2) COVAR_POP()
			- Calculates the population covariance between two sets of values.
		3) COVAR_SAMP()
			- Calculates the sample covariance between two sets of values.

	- Distribution Window Functions:
		1) PERCENTILE_CONT()
			- Calculates the interpolated value at a specified percentile in a group.
		2) PERCENTILE_DISC()
			- Calculates the value at a specified percentile in a group.

	- Linear Regression Window Functions:
		- Perform linear regression calculations on a set of data points within a window.
		1) REGR_SLOPE()
			- Calculates the slope of the linear regression line for a set of data points within a window.
			- It returns the slope as a floating-point number.
		2) REGR_INTERCEPT()
			- Calculates the y-intercept of the linear regression line for a set of data points within a window.
			- It returns the y-intercept as a floating-point number.
		3) REGR_COUNT()
			- Counts the number of non-null pairs of values used in the linear regression calculation within a window.
			- It returns the count as an integer.
		4) REGR_R2()
			- Calculates the coefficient of determination (R-squared) for the linear regression line within a window.
			- R-squared indicates the proportion of the variance in the dependent variable that can be explained by the independent variable.
			- It returns the R-squared value as a floating-point number between 0 and 1.
		5) REGR_AVGX()
			- Calculates the average of the independent variable (X) values within a window for the linear regression calculation.
			- It returns the average X value as a floating-point number.
		6) REGR_AVGY()
			- Calculates the average of the dependent variable (Y) values within a window for the linear regression calculation.
			- It returns the average Y value as a floating-point number.
		7) REGR_SXX()
			- Calculates the sum of squares of the independent variable (X) values within a window for the linear regression calculation.
			- It returns the sum of squares as a floating-point number.
		8) REGR_SYY()
			- Calculates the sum of squares of the dependent variable (Y) values within a window for the linear regression calculation.
			- It returns the sum of squares as a floating-point number.
		9) REGR_SXY()
			- Calculates the sum of the products of the independent variable (X) values and dependent variable (Y) values within a window for the linear regression calculation.
			- It returns the sum of products as a floating-point number.

- rownumber vs rank vs denserank
	- ROW_NUMBER() assigns a unique row number to each row based on the order specified, sequentially.
	- RANK() assigns a unique rank to each row based on the order specified.
		- If two rows have same values and are assigned the same rank, the next rank will be skipped.
		- Ex:
			SELECT Name, Marks, RANK() OVER (ORDER BY Marks DESC) AS Rank FROM Students

			Name	Marks	Rank
			--------------------
			Bob		92		1
			David	92		1
			Alice	85		3
			Emma	85		3
			Charlie	78		5

	- DENSE_RANK() assigns a unique rank to each row based on the order specified.
		- If two rows have same values, they are assigned the same rank, and there are no gaps in ranking values.
		- Ex:
			SELECT Name, Marks, DENSE_RANK() OVER (ORDER BY Marks DESC) AS DenseRank FROM Students

			Name	Marks	DenseRank
			-------------------------
			Bob		92		1
			David	92		1
			Alice	85		2
			Emma	85		2
			Charlie	78		3

	- CONCLUSION:
		ROW_NUMBER() -- 1,2,3,4,5 (different rank no. to same value)
		RANK() 		 -- 1,2,2,4,5 (same rank no. to same value but next rank is skipped)
		DENSE_RANK() -- 1,2,2,3,4 (same rank no. to same value but no skip in rank no.)

• Two ways of writing SQL Queries
	a) Using comma separator
		- This approach involves listing multiple tables in the FROM clause separated by commas and applying join conditions in the WHERE clause.
		- ex:
				SELECT *
				FROM Table1, Table2
				WHERE Table1.id = Table2.id;
		- Pros:
			- Simple and straightforward syntax.
			- Works well for joining a small number of tables with simple join conditions.
			- Can be easier to read and understand for basic queries.
		- Cons:
			- Limited flexibility in expressing complex join conditions or performing different types of joins (e.g., inner join, outer join).
			- May lead to Cartesian product (unintended duplication of rows) if join conditions are not specified correctly.
			- May result in longer query execution time for large tables or complex queries due to inefficient join operations.

	b) Using JOIN
		- In this approach, tables are joined using explicit JOIN clauses (e.g., INNER JOIN, LEFT JOIN, etc.) in the FROM clause, specifying the join conditions.
		- Pros:
			- Offers more flexibility in expressing various types of joins and complex join conditions.
			- Provides better readability and maintainability, especially for complex queries involving multiple tables.
			- Can result in better query optimization and performance when used appropriately.
		- Cons:
			- Requires a good understanding of join types and syntax.
			- May result in more verbose queries compared to the comma separator approach.
			- Incorrect usage of joins or missing join conditions can lead to incorrect results.

- SQL JOINs
	- https://insightoriel.com/what-is-join-in-sql/
	- https://hackr.io/blog/sql-server-interview-questions
	- Four types -  left, right, inner, and outer.
	1) (INNER) JOIN 		 : Returns records that have matching values in both tables.
	2) LEFT (OUTER) JOIN  : Returns all records from the left table, and the matched records from the right table.
	3) RIGHT (OUTER) JOIN : Returns all records from the right table, and the matched records from the left table.

	4) FULL (OUTER) JOIN	 : Returns all rows from both tables, matching rows based on the join condition. If a row in one table does not have a match in the other table, the result will contain NULL values for the unmatched columns.
		-- MySql FULL OUTER JOIN

				SELECT * FROM t1
				LEFT JOIN t2 ON t1.id = t2.id
				UNION ALL
				SELECT * FROM t1
				RIGHT JOIN t2 ON t1.id = t2.id
				WHERE t1.id IS NULL

	5) CROSS(or CARTESIAN) JOIN : there is no ON clause here. Each row from first table is combined with each row from second table.

		select * from employees, departments

		SELECT w.name AS wine, m.name AS main_course
			FROM wine w
			CROSS JOIN main_course m;

	6) SELF JOIN				: Inner join of table, with same table with different alias. Useful when a table has a hierarchical structure or when comparing rows within the same table.

		- Assume a table contain all employees and their manager information.
		- Then, we can use this self join to get the employees and their manager names
			Employee table  has three columns - id, name, manager_id

			SELECT
				e.name as employee_name,
				m.name as manager_name
			FROM employee e
			INNER JOIN employee m  ON e.id = m.id

	7) NATURAL JOIN 	: Matches columns with the same name in both tables, without requiring an explicit join condition. However, it is less commonly used due to the potential for unexpected matches. (Oracle, PostgreSQL, MySQL, SQLite)

	8) CROSS APPLY		: Applies a table-valued function to each row of another table, producing a result set that combines the rows from both.(SQL Server)

	9) OUTER APPLY 		: Similar to Cross Apply, but also includes unmatched rows from the outer table.(SQL Server)

- Number of records in each JOIN
		TableA - 10 rows
		TableB - 20 rows

		INNER JOIN 				- Equal to matching number of records, based on condition
		LEFT(OUTER) JOIN 		- (10) Equal to number of records in LEFT table
		RIGHT(OUTER) JOIN 		- (20) Equal to number of records in RIGHT table
		FULL (OUTER) JOIN 		- (10 + 20) Equal to summation of number of records in all tables
		CROSS (CARTESIAN)JOIN	- (10 * 20) Equal to products of number of records in all tables
		SELF JOIN				- Depends on matching values within same table based on join condition
		NATURAL JOIN			- Depend on columns with same name in both/all tables

		Also,
		Left joins can increase number of rows in left table if there are multiple matches in right table. Same goes for other joins too.

- FULL OUTER JOIN Vs UNION
	- Full outer join combines the rows from two tables based on a common column, including unmatched rows, and provides a combined result set.
	- Union and union all, combines the result sets of SELECT statements, regardless of any common columns between the statements.
	- Ex:
			SELECT *
			FROM Customers
			FULL OUTER JOIN Orders ON Customers.customer_id = Orders.customer_id;

			SELECT column1, column2, column3 FROM TableA
			UNION
			SELECT column1, column2, column3 FROM TableB;

- Normalization
	- To divide large tables into smaller tables and defining relationships between them, to minimize the
		redundancy of data.

	1) First Normal Form (1NF):
		- Duplicate columns from the same table needs to be eliminated.
		- We have to create separate tables for each group of related data and
		identify each row with a unique column or set of columns (Primary Key)

	2) Second Normal Form (2NF):
		- First it should meet the requirement of first normal form.
		- Removes the subsets of data that apply to multiple rows of a table and
		place them in separate tables.
		- Relationships must be created between the new tables and their predecessors
		through the use of foreign keys.

	3) Third Normal Form (3NF):
		- First it should meet the requirements of the second normal form.
		- Remove columns that are not depending upon the primary key.
		- Most databases will be Third Normal Form

	4) Fourth Normal Form (4NF):
		- There should not be any multi-valued dependencies.
		- Means, each column in a table is dependent on primary key and not on other non-key columns.

- Relations
	1) One-to-One (1:1) Relationship:
		- A row in one table is related to only one row in another table, and vice versa.
		- Example:
			- A person and their passport information.
			- Each person has only one passport, and each passport belongs to only one person.

	2) One-to-Many (1:N) Relationship:
		- A row in one table is related to multiple rows in another table, but each row in the second table is related to only one row in the first table.
		- Example:
			- An order and its line items.
			- Each order can have multiple line items, but each line item belongs to only one order.

	3) Many-to-One (N:1) Relationship:
		- Multiple rows in one table are related to a single row in another table.
		- Example:
			- Many employees working in the same department.
			- Multiple employees can belong to a single department.

	4) Many-to-Many (N:N) Relationship:
		- Multiple rows in one table can be related to multiple rows in another table.
		- Example:
			- Students and courses.
			- Multiple students can enroll in multiple courses, and each course can have multiple students.

	5) Self-Referencing Relationship:
		- A table can have a relationship with itself, where rows in the table are related to other rows within the same table.
		- Example:
			- An employee table with a "manager_id" column that references the primary key of another row in the same table to represent a hierarchical employee structure.

	6) Junction Table (or Associative Table):
		- Used to establish a many-to-many relationship by creating an intermediary table that holds the associations between the two related tables.
		- Example:
			- A table to associate students with courses they are enrolled in, containing student IDs and course IDs.

Transaction Table
	- Used to store individual transactions or events in a database
	- Used to maintain an audit trail or track changes over time.
	- Primary purpose is to provide a log or record of actions performed within the system.
	- It focuses on recording details of each transaction, such as transaction date, time, participants, and relevant data associated with that specific event.

- Bridge table
	- A bridge table is used to connect two or more tables that have a many-to-many relationship.
	- It typically contains foreign keys to the primary keys of the other tables, and each row in the table represents a relationship between the data in those tables.
	- The bridge table is used to establish a many-to-many relationship between the tables, and allows queries to be performed that involve both sets of data.


- DELETE
	- DML COMMAND
	- Delete Rows from the table one by one
	- We can use where clause with Delete to delete single row
	- Delete is slower than truncate
	- ROLLBACK is possible with DELETE

- TRUNCATE
	- DDL COMMAND
	- Truncate deletes rows at a one goal
	- We can't use where clause with Truncate
	- Truncate faster than both DELETE & DROP
	- Rollback is not possible with Truncate

- DROP
	- DDL COMMAND
	- Delete the entire structure or schema
	- We can't use where clause with drop
	- Drop is slower than DELETE & TRUNCATE
	- ROLLBACK IS NOT POSSIBLE WITH DROP

-  Truncate vs Delete
	- Both used to delete data from the table
	- Truncate is a DDL statement. Delete is a DML statement
	- Truncate does not generate rollback segments. Whereas Delete does.
	- In case of delete, rollback recovers data before issuing a commit.
		In case of truncate, you cannot recover data.
	- Truncate does not fire any delete triggers created on the table.
		Whereas the delete does.

- COALESCE
	- it will return the first non-null value, in the column
	- Also, We can use it to replace null values in a join with some default value
		COALESCE(field, 'default')

- Having vs Where
	- HAVING clause is used with the GROUP BY clause to filter grouped records based on aggregate functions.
	- It operates on the result of the grouping.
	- filters based on the result of aggregate functions, such as MIN, MAX, SUM, COUNT, etc.

	- WHERE clause is used to filter individual records before any grouping or aggregation occurs.
	- It applies to the raw data.
	- WHERE clause cannot filter aggregated records. HAVING is a column operation.


- Union vs Union all
	- Union set operator removes duplicate records. Whereas union all does not.
	- Union operator sorts the data in ascending order. union all does not.
	- Union all is faster than union operator.

- Intersect vs Except
	- INTERSECT: Takes the data from both result sets which are in common.
	- EXCEPT: Takes the data from the first result set, but not in the second
			  result set (i.e. no matching to each other)

- Subquery
	- subquery is also called as nested or inner query.
	- It is a query nested within another query.
	- subquery can be used in various parts of a SQL statement, such as SELECT, FROM, WHERE, HAVING, JOIN.
	- A query can contain more than one sub-query.

	- Constraints of Subqueries:
		- A sub-query must be enclosed in the parenthesis.
		- A sub-query cannot contain an ORDER-BY clause.
		- A sub-query must be put in the right hand of the comparison operator, and
		- The result of a subquery should be compatible with the operator or condition where it is used.

	- Types
		1) Scalar subquery
			- A scalar subquery is a subquery that returns a single value.
			- It can be used in expressions or conditions where a single value is expected, such as SELECT, WHERE, or HAVING clauses.
			- Example: SELECT name FROM employees WHERE salary > (SELECT AVG(salary) FROM employees)

		2) Single-row sub-query
			- A single-row subquery is a subquery that returns a single row with one or more columns.
			- It can be used in conditions where a comparison is made with a single row, such as WHERE or HAVING clauses.
			- Example: SELECT name FROM employees WHERE department_id = (SELECT department_id FROM departments WHERE name = 'IT')

		3) Multiple-row Subquery
			- A multiple-row subquery is a subquery that returns multiple rows with one or more columns.
			- It can be used in conditions where multiple rows are compared or when the IN or ANY/ALL operators are used.
			- Example: SELECT name FROM employees WHERE department_id IN (SELECT department_id FROM departments WHERE location = 'New York')

		4) Multiple-Column Subquery:
			- A multiple-column subquery is a subquery that returns multiple columns.
			- It can be used in situations where we need to retrieve multiple columns from a subquery to be used in the outer query.
			- The result of the subquery is treated as a table or a derived table, and we can reference its columns in the outer query.
			- Example: SELECT name, department FROM employees WHERE (department, salary) IN (SELECT department, MAX(salary) FROM employees GROUP BY department)

		5) Correlated Subquery:
			- A correlated subquery is a subquery that refers to a column from the outer query.
			- The subquery is evaluated once for each row processed by the outer query.
			- It can be used to filter the results based on values from the outer query.
			- Example: SELECT name FROM employees e WHERE salary > (SELECT AVG(salary) FROM employees WHERE department_id = e.department_id)
		- Multiple column sub-query, where the sub-query returns multiple columns.


- Subquery vs JOIN vs CTE
	- subqueries can be used, when we need to perform calculations or filtering based on the results of another query. They are flexible, easy to understand; but can impact performance when dealing with large data.
	- JOINs can be used to combine related data from multiple tables. They provide good performance and expressiveness but can be complex.
	- Use CTEs to simplify complex queries and enhance code readability. They are reusable but have limited scope and require optimization.


- CTE
	- Common Table Expressions (CTEs) are temporary result sets that are defined within a query and can be referenced multiple times within that query.
	- CTEs makes code readable, by breaking down logic, to managable parts.
	- CTEs can also be used as views.
	- CTE's members cannot use the clauses of keywords like Distinct, Group By, Having, Top,
	  Joins limiting by this type of the queries that can be created and reducing their complexity.

	- TYPES
		1) Non-Recursive CTE:
			- It does not refer to itself within its definition.
			- The CTE is defined only once and used as a derived table in the subsequent query.
			- ex:
						WITH cte AS (
						  SELECT * FROM employees WHERE salary > 5000
						)
						SELECT * FROM cte WHERE department = 'IT';

		2) Recursive CTE:
			- A recursive CTE, also known as a hierarchical or self-referencing CTE, refers to itself within its definition.
			- It allows for recursive queries, which are useful for handling hierarchical or nested data structures.
			- Recursive CTEs require two parts: the anchor member and the recursive member.
				= anchor member		: defines the base case or initial rows, and
				= recursive member 	: defines the subsequent iterations or recursive steps.
			- The recursion continues until a termination condition is met.
			- Ex:
						WITH RECURSIVE cte (id, name, manager_id) AS (
						  SELECT id, name, manager_id FROM employees WHERE id = 1
						  UNION ALL
						  SELECT e.id, e.name, e.manager_id FROM employees e
						  INNER JOIN cte ON e.manager_id = cte.id
						)
						SELECT * FROM cte;


						WITH odd_num_cte (id, n) AS (
							SELECT 1, 1
							UNION ALL
							SELECT id+1, n+2 from odd_num_cte where id < 5
						)
						SELECT * FROM odd_num_cte;

- Temporary Tables:
	- Definition:
		- Temporary tables are physical tables that are created and exist for a specific session or transaction.
		- They are created using the CREATE TABLE statement and can be modified like regular tables.
	- Scope:
		- Temporary tables have a broader scope and can be accessed and manipulated within a session or transaction until they are explicitly dropped or the session ends.
	- Usage:
		- Temporary tables are useful for storing intermediate results, joining complex queries, and performing multi-step data transformations or aggregations.
	- Syntax:
		- Temporary tables are created using the CREATE TABLE statement with the addition of the keyword "TEMPORARY" or "TEMP" before the table name.
	- Ex:
						CREATE TEMPORARY TABLE temp_employees AS
						SELECT * FROM employees WHERE salary > 5000;

						SELECT * FROM temp_employees WHERE department = 'IT';

						DROP TABLE temp_employees;

	- temp tables types
		- Table variables (DECLARE @t TABLE)
			- visible only to the connection that creates it, and
			- are deleted when the batch or stored procedure ends.

		- Local temporary tables (CREATE TABLE #t)
			- visible only to the connection that creates it, and
			- are deleted when the connection is closed.

		- Global temporary tables (CREATE TABLE ##t)
			- visible to everyone, and
			- are deleted when all connections that have referenced them have closed.

		- Tempdb permanent tables (USE tempdb CREATE TABLE t)
			- visible to everyone, and
			- are deleted when the server is restarted.


- CTE vs temptables
	- CTEs are preferable when we need a temporary result set within a specific query,
	= temporary tables are suitable for larger or more persistent intermediate result sets that need to be accessed across multiple queries or sessions.

- temp table or table variable
	- table variable is faster than the temporary table.
	- Temporary tables are allowed CREATE INDEXes whereas, Table variables aren't allowed CREATE INDEX instead they can have an index by using Primary Key or Unique Constraint.

	- Table variable (@table) is created in the memory, Whereas, Temporary table (##temp/#temp) is created in tempdb database.
	- However, if there is a memory pressure the pages belonging to a table variable may be pushed to tempdb.
	- Table variables cannot be involved in transactions, logging or locking.


- DB view vs CTE
	- CTE cannot be nested while Views can be nested.
	- View once declared can be used for any number of times but CTE cannot be used.
	It should be declared every time you want to use it.

- In-memory data structure store databases
	- They are well-suited for applications that require fast access to frequently accessed data and low latency data processing.
	- They can provide high performance and scalability for applications that need to handle large volumes of data and support real-time data processing.
	- Ex: Redis, Memcached, Apache Ignite, Aerospike


- DB View vs Materialized View
	DB Views														|Materialized DB Views
	----------------------------------------------------------------------------------------------
	Virtual table and doesn’t occupy any storage space				|Copies of data are stored in the memory
	It will have up to date data in it as it is executed at run-time|Needs to be refreshed every time when it is used as it has compiled data
	Executed when a query is run n view using SELECT				|Executed and records are stored in the database
	Data access is slower	 										|Faster data access because data is directly accessed from physical location
	Generally, used to restrict data from database					|Generally, used in data warehousing


- Cursor
	- Used when application logic need to work with one row at a time, rather than entire result at once.
	- Can be used
		- To iterate over a large number of records.
		- To perform complex operations on a set of records.
		- To process records in a specific order.
	- Types
		1) Forward-only cursors	: can only be traversed in the forward direction.
		2) Bidirectional cursors: can be traversed in both the forward and backward directions.
	- Advatanges
		- Cursors provide first few rows before the whole result set is assembled.
		  so, better response time is achived, compared to traditional queries, without cursor.
	- Limitations
		- Uses more resource because each time you fetch a row from cursor, it results in
		  a network roundtrip.
		- Because of the round trips, performance and speed is slow
	- Best Practices
		- Use cursors only when necessary.
		- Use the most efficient type of cursor for the task at hand.
		- Close cursors when you are finished with them.

	- Alternatively, we can use JOIN to retrieve the data much fastly

- Stored procedure vs Functions
	- procedure allows SELECT as well as DML(INSERT/UPDATE/DELETE) statement in it
	  whereas Function allows only SELECT statement in it.
	- Procedures cannot be utilized in a SELECT statement whereas Function can be
	  embedded in a SELECT statement.
	- Stored Procedures cannot be used in the SQL statements anywhere in the
	  WHERE/HAVING/SELECT section whereas Function can be.
	- An exception can be handled by try-catch block in a Procedure whereas
	  try-catch block cannot be used in a Function.
	- stored procs neither contain any parameter, nor return any value.
	  Functions should contain atleast one parameter and should return a value.
	- functions can be called from stored procs; but cant call stored procs from functions.
	- Transactions and DML commands can be executed on stored procs; but not with functions.

- Stored procedure vs DB triggers
	- Trigger is a procedure invoked automatically when certain db events(like create/delete record) occurs.
	  Stored Procedure is a procedure that is explicitly invoked by either
		- another client application
		- or another stored procdure
		- or another trigger procedure
	- Also, Stored procedures can accept parameters  and can return values, whereas
	  triggers can neither accept parameters nor return values.
	- And, transaction statements cant be used inside a trigger; whereas transaction statements
	  like begin transaction, commit transaction and rollback, can be used in stored procedures.

- User-defined Functions Vs Stored Procedures
	- The function must return a value but in Stored Procedure it is optional. Even a procedure can return zero or n values.
	- Functions can have only input parameters for it whereas Procedures can have input or output parameters.
	- Functions can be called from Procedure whereas Procedures cannot be called from a Function.

- DB Indexing
	- Clustered
		- sort and store the data rows in the table, or view based on their key values.
		- index contains pointers to block, but not direct data.
		- Ex:if primary key is applied to any column, it automatically becomes clustered index.
		- You can have only one clustered index in one table, but you can have one clustered
		  index on multiple columns, and that type of index is called composite index.
		- faster and requires less memory for operations
	- Non-clustered
		- The data is stored in one place, and index is stored in another place.
		  Since, the data and non-clustered index is stored separately, then you can have
		  multiple non-clustered index in a table.
		- In non-clustered index, index contains the pointer to data.
		- slow and requires more memory for operations

- physical index vs logical index
	- physical index is the actual indexing structure as it is stored on disk.
	- logical index is a reference to a physical index.

	- When we create a primary key, secondary key, foreign key, or unique constraint, the
	  database server ensures referential integrity by creating a logical index for the constraint.

- Sharding
	- It is a technique that splits data into separate rows and columns held on separate database
	server instances in order to distribute the traffic load.
	- Each small table is called a shard.
	• Sharding key: a specific column value that indicates which shard this row is stored in.
	• Sharding algorithm: an algorithm to distribute your data to one or more shards.


- Partition tables
	- Partitioning tables is a way to divide a large table into smaller, more manageable pieces.
	- This can improve query performance by reducing the amount of data that needs to be scanned.
	- To partition a table in PostgreSQL, you can use the CREATE TABLE command with the PARTITION BY clause.
	- This clause specifies the partitioning method and the partition key

	- Partition types
		1) Range partitioning	: partitions are defined based on a range of values for the partition key.
		2) Hash partitioning	: partitions are determined based on a hash function applied to the partition key.
		3) List partitioning	: partitions are defined based on a list of values for the partition key.

	- considerations when partitioning tables
		- Make sure your partition key is carefully chosen, as it will determine the efficiency of partitioning.
		- Be aware of potential data skew, where some partitions are much larger than others.
		- Be careful when performing operations that affect multiple partitions, such as altering the partition key or merging partitions, as these can cause deadlocks.


- DB Triggers
	- they are kind of stored procedures, which reacts to certain actions we make in db.
	- actions like creating/updating/deleting a record
	- TYPES
		- DML (data manipulation language) triggers
			- fired in response to data manipulation ops such as INSERT, UPDATE or DELETE statements on a table.
		- DDL (data definition language) triggers
			- fired in response to changes in the database schema or structure.
			- react to  CREATE, ALTER, and DROP operations
			- used to enforce data integrity rules or audit schema changes.
		- Logon triggers
			- reacts to LOGON events; means fired when a user successfully logs in to the database.
			- used to enforce security policies or perform certain actions upon user login.

	ref: https://www.sqlshack.com/learn-sql-sql-triggers/


- N+1 problem
	- This problem occurs in ORMs.
	- The problem occurs when you have a query that loads a set of objects, and then each object requires an additional query to load its associated data.
	  If you have N objects, this can result in N+1 queries being executed against the database, where the first query retrieves the N objects, and then one additional query is executed for each of the N objects to retrieve the associated data.
	- In Django, we can use the select_related and prefetch_related methods to address the N+1 problem and improve the performance of your database queries.

- SYSDATE vs CURRENT_DATE
	- SYSDATE returns the current date and time set for the operating system of the database server.
	- CURRENT_DATE returns the current date in the session time zone.

- SYSDATE vs NOW
	- SYSDATE() returns the time at which it executes.
	- NOW() returns a constant time that indicates the time at which the statement began to execute.
	- Example - Within a stored routine or trigger, NOW() returns the time at which the routine or triggering statement began to execute.

- row-store vs column-store
	- row store
		- easy to add/modify a record
		- might read in unnecessary data
	- column-store
		- only need to read in relevant data
		- tuple writes require multiple accesses
		- suitable for read-mostly, read-intensive, large repositories

- SQL vs no-SQL
	- Mainly sql has schema, where as nosql doesnt have it.
	- SQL
		- Data is stored in tables with mutable but pre-defined Columns. Records are stored as Rows in these tables.
		- Implements the ACID database model, which basically means it favours consistency over availability.
		- Scalability is mainly achieved by increasing the resources on the host, usually this means more cpu, memory, storage or network capacity(vertically scalable).
	- NoSQL
		- Data can be stored as documents, key-value pairs, tables and graphs.
		- Implements the BASE database model, which basically means it favours availability over consistency.
		- Scalability is mainly achieved by spreading datasets across multiple servers.(horizontally scalable)

- MongoDB - https:
	- cheatography.com/zeineb-and-kawther/cheat-sheets/mongodb/

		SQL						|    NoSQL
		------------------------------------------------
		  relational			| non-relational
								|
		use structured query	| NoSQL databases have
		language and have a		| dynamic schemas for
		predefined schema.		| unstructured data.
								|
		are vertically scalable | are horizontally scalable.
		are table based			| are document, key-value,
								| graph or wide-column stores.

	- joins in mongodb is with lookups
		ex: If we have two collections: orders and customers, we can join as
			db.orders.aggregate([
			  {
				$lookup: {
				  from: "customers",
				  localField: "customer_id",
				  foreignField: "_id",
				  as: "customer"
				}
			  }
			])

- BASE model
	- Basically Available
		– Rather than enforcing immediate consistency, BASE-modelled NoSQL databases will
		  ensure availability of data by spreading and replicating it across the nodes of
		  the database cluster.
	- Soft State
		– Due to the lack of immediate consistency, data values may change over time.
		- The BASE model breaks off with the concept of a database which enforces its own
		  consistency, delegating that responsibility to developers.
	- Eventually Consistent
		– The fact that BASE does not enforce immediate consistency does not mean that it
		  never achieves it.
		- However, until it does, data reads are still possible (even though they might
		  not reflect the reality).

Database Design Principles
- Good database design is driven by several core principles:
	1) Minimize redundancy
		- To save resources, create an efficient database, and simplify how the database works, data redundancy is minimized and duplication is avoided.
	2) Protect accuracy
		- Your database should keep information accurate and reduce the likelihood of accidentally damaging information.
	3) Be accessible
		- The business intelligence systems that need reading and writing access should have it.
		- Your database should provide access while also protecting data security.
	4) Meet expectations
		- Of course, you keep a database to fulfill a specific purpose—so the database design must successfully support your data processing expectations.


- SQL performance Tuning
	1. Use EXISTS instead of IN to check the existence of data.
	2. Avoid * in the SELECT statement. Give the name of the columns which you require.
	3. Choose appropriate Data Type. E.g. To store strings use varchar in place of text data type. Use text data type,
	   whenever you need to store large data (more than 8000 characters).
	4. Avoid nchar and nvarchar if possible since both the data types take just double memory as char and varchar.
	5. Avoid NULL in fixed-length field. In case of requirement of NULL, use variable-length (varchar) field that takes less space for NULL.
	6. Avoid Having Clause. Having clause is required if you further wish to filter the result of an aggregations.
	7. Create Clustered and Non-Clustered Indexes.
	8. Keep clustered index small since the fields used in clustered index may also used in non-clustered index.
	9. Most selective columns should be placed leftmost in the key of a non-clustered index.
	10. Drop unused Indexes.
	11. Better to create indexes on columns that have integer values instead of characters. Integer values use less overhead than character values.
	12. Use joins instead of sub-queries.
	13. Use WHERE expressions to limit the size of result tables that are created with joins.
	14. Use TABLOCKX while inserting into a table and TABLOCK while merging.
	15. Use WITH (NOLOCK) while querying the data from any table.
	16. Use SET NOCOUNT ON and use TRY- CATCH to avoid deadlock condition.
	17. Avoid Cursors since cursor are very slow in performance.
	18. Use Table variable in place of Temp table. Use of Temp tables required interaction with TempDb database which is a time taking task.
	19. Use UNION ALL in place of UNION if possible.
	20. Use Schema name before SQL objects name.
	21. Use Stored Procedure for frequently used data and more complex queries.
	22. Keep transaction as small as possible since transaction lock the processing tables data and may results into deadlocks.
	23. Avoid prefix “sp_” with user defined stored procedure name because SQL server first search the user defined procedure in the master database and after that in the current session database.
	24. Avoid use of Non-correlated Scalar Sub Query. Use this query as a separate query instead of part of the main query and store the output in a variable, which can be referred to in the main query or later part of the batch.
	25. Avoid Multi-statement Table Valued Functions (TVFs). Multi-statement TVFs are more costly than inline TVFs.
	26. optimize MYSQL field type and sizes,
		eg. use INT instead of VARCHAR if data can be presented with numbers,
		    use SMALL INT instead of BIG INT, etc.
		In case we need to have VARCHAR, then use as small as possible length of each field,
	27. MYSQL partitioning can be implemented with some kind of partitioning scheme,
		e.g. new partition is created by CRONJOB every day at "night" when server utilization is at minimum,
			or when you reach another 50k INSERTs or so
		(by the way, also some extra effort will be needed for UPDATE/DELETE operations on different partitions),

	28. caching is another very simple and effective approach, for repeatedly queried results
	29. in-memory database can be used alongside tradtional SQL DBs to get often-needed data:
		simple key-value pair style could be enough: Redis, Memcached, VoltDB, MemSQL are just some of them. Also, MYSQL also knows in-memory engine,


- SQL performance of inserts
	1) Remove all triggers and constraints on the table
	2) Remove all indexes, except for those needed by the insert
	3) Ensure your clustered index is such that new records will always be inserted at the end of the table
	 (an identity column will do just fine). This prevents page splits (where SQL Server must move data around because an existing page is full)
	4) Set the fill factor to 0 or 100 (they are equivalent) so that no space in the table is left empty, reducing the number of pages that the data is spread across.
	5) Change the recovery model of the database to Simple, reducing the overhead for the transaction log.


- SQL Queries
	SELECT SYSDATE();								-- 2021-07-13 06:12
	SELECT DATE_ADD(SYSDATE(), INTERVAL 1 DAY);		-- 2021-07-14 06:12
	SELECT DATE_ADD(SYSDATE(), INTERVAL -1 DAY);	-- 2021-07-12 06:12
	SELECT DATE(SYSDATE());							-- 2021-07-13 00:00

	SELECT DATE_SUB("2017-06-15", INTERVAL 10 DAY);

	SELECT DATE_ADD(CURDATE(), INTERVAL 1 DAY) AS tomorrow_date;
	SELECT DATE_SUB(CURDATE(), INTERVAL 2 MONTH) AS date_two_months_ago;


Characteristics of MySQL:
	- Database locking (MUCH easier for financial transactions)
	- Consistency/security (as above, you can guarantee that, for instance, no changes happen between the time you read a bank account balance and you update it).
	- Data organization/refactoring (you can have disorganized data anywhere, but MySQL is better with tables that represent "types" or "components" and then combining them into queries -- this is called normalization).
	- MySQL (and relational databases) are more well suited for arbitrary datasets and requirements common in AGILE software projects.

	- Two db engines
		- MyISAM - My Indexed Sequential Access Method
			- used for web, data warehousing and other analytic environments
	- InnoDB vs MyISAM
		- InnoDB has row-level locking; MyISAM only has full table level locking
		- InnoDB supports transactions, means we can commit and rollback
		  MyISAM is faster to read
		- InnoDB supports foreign keys and relationship constraints(referential integrity), where as MyISAM doesnt support.
		- InnoDB supports caching both data and indexes, but doesnt support full-text search.
		  MyISAM is only meant for indexes, and full-text search.
		- InnoDB supports transaction logs and data recovery on failure; whereas MyISAM doesnt.

Characteristics of Cassandra:
	- Speed: For simple retrieval of large documents. However, it will require multiple queries for highly relational data – and "by default" these queries may not be consistent (and the dataset can change between these queries).
	- Availability: The opposite of "consistency". Data is always available, regardless of being 100% "correct".[1]
	- Optional fields (wide columns): This CAN be done in MySQL with meta tables etc., but it's for-free and by-default in Cassandra.

Tombstones in Cassandra:
	- In cassandra, deleted data is not immediately purged from the disk.
	- Instead, Cassandra writes a marker value, called Tombstone, to indicate that data has been deleted.
	- The tombstone is necessary, as distributed data stores use eventual consistency, where only a subset of nodes where the data is stored must respond before an operation is considered to be successful.
	- The grace period for a tombstone is set by the property gc_grace_seconds. Its default value is ten days.

Postgresql
==========
	- It is ACID compliant
	- It supports Multi-Version Concurrency Control (MVCC)
	- It supports user-defined data types
	- Also, it supports audio, video, images and graphical data storage

data types
	Boolean
	Character Types [ such as char, varchar, and text]
	Numeric Types [ such as integer and floating-point number]
	Temporal Types [ such as date, time, timestamp, and interval]
	UUID [ for storing UUID (Universally Unique Identifiers) ]
	Array [ for storing array strings, numbers, etc.]
	JSON [ stores JSON data]
	hstore [ stores key-value pair]
	Special Types [ such as network address and geometric data]

INITCAP()
	make the first letter of each word uppercase and the rest of the letters lowercase.

type Conversion
	1) using CAST ( expression AS target_type );
			CAST ('100' AS INTEGER);
			CAST ('2015-01-01' AS DATE),
			CAST ('01-OCT-2015' AS DATE);
			CAST ('10.2' AS DOUBLE PRECISION);
			CAST('true' AS BOOLEAN),
			CAST('false' as BOOLEAN),
			CAST('T' as BOOLEAN),
			CAST('F' as BOOLEAN);

	2) using expression::type
			'100'::INTEGER,
			'01-OCT-2015'::DATE;
			'2019-06-15 14:30:20'::timestamp;
			'15 minute'::interval,
			'2 hour'::interval,
			'1 day'::interval,
			'2 week'::interval,
			'3 month'::interval;

TRIM
	LTRIM() function removes all characters, spaces by default, from the beginning of a string.
	RTRIM() function removes all characters, spaces by default, from the end of a string.
	BTRIM function is the combination of the LTRIM() and RTRIM() functions.

	Syntax:
		TRIM(LEADING character FROM string); -- LTRIM(string,character)
		TRIM(TRAILING character FROM string); -- RTRIM(string,character)
		TRIM(BOTH character FROM string); -- BTRIM(string,character)

	REGEX_REPLACE
		SELECT REGEXP_REPLACE('enterprise 	', '\s+$', '');

postgresql string functions - https://www.w3resource.com/PostgreSQL/introduction-to-postgresql-string-functions.php

MongoDb
=======
	// Join in mongoDB

	// Customer table
	db.customer.insertMany(
		[
			{ "name": "Abhishek", "age": 50, "address": "f/403 anurag", productId: 1 },
			{ "name": "raj", "age": 30, "address": "302 Neelmani", productId: 2 },
			{ "name": "simran", "age": 70, "address": "rajkot", productId: 3 },
			{ "name": "Dhruval", "age": 60, "address": "chennai", productId: 4 }
		]
	)

	// Product table
	db.product.insertMany(
		[
			{ "product_name": "Abhishek", prod_id: 2, "price": 300 },
			{ "product_name": "raj", prod_id: 3, "price": 500 },
			{ "product_name": "simran", prod_id: 1, "price": 400 },
			{ "product_name": "Dhruval", prod_id: 4, "price": 1000 }
		]
	)

	// Join using lookUp
	db.customer.aggregate([
		{
			$lookup:
			{
				from: "product",
				localField: "productId",
				foreignField: "prod_id",
				as: "productReference"
			}
		}
	])

sharding vs replication
	- Replication is duplication of dataset, whereas sharding is patitioning of dataset, into parts.
	- Sharding helps in horizontal scaling of data writes by partitioning data across multiple servers using a shard key.
	  Replication helps with horizontal scaling of reads, but data may not be latest, on all copies.
	- Replication helps in increasing data availability; acts as backup, incase primary node fails.

	- Full-time replication will add burden on network, and takes more time, if dataset is large.

Data Replication Types
	1) Full table replication
	2) Transactional replication
	3) Snapshot replication
	4) Merge replication
	5) Key-based incremental replication

replication vs backup
	- backup means maintaining historic state record; adds versioning.
	- replication will update the existing data, on an ongoing basis.

Sharding Types
	1) key(or Hash) based sharding
	2) Range based sharding
	3) Directory based sharding

	https://segwitz.com/what-is-database-sharding/

MongoDb ObjectId structure
	ObjectId consists of 12-byte:
		- A 4-byte timestamp, representing the ObjectId's creation, measured in seconds since the Unix epoch.
		- A 5-byte random value generated once per process. This random value is unique to the machine and process.
		- A 3-byte incrementing counter, initialized to a random value.
	If an integer value is used to create an ObjectId, the integer replaces the timestamp.


covered query
	It is a query in which − All the fields in the query are part of an index.
	All the fields returned in the query are in the same index.
	Covered queries help in querying data faster.
	It is achieved by ensuring the index created contains all the fields required by the query.
	It doesn't require examining any documents apart from the indexed ones.
	We need to ensure that all the fields in the query, as well as results returned, are part of the index.


MongoDb query optimization/Optimize Query Performance
	1) Create Indexes to Support Queries.
	2) Use the Increment Operator to Perform Operations Server-Side.
	3) Use Projections to Return Only Necessary Data.
	4) Use $hint to Select a Particular Index.
	5) Limit the Number of Query Results to Reduce Network Demand.

DB2
====
	ibm_db to connect to DB2

	import ibm_db
	conn = ibm_db.connect("database","username","password")

	conn = ibm_db.connect("DATABASE=name;HOSTNAME=host;PORT=60000;PROTOCOL=TCPIP;UID=username;
                PWD=password;", "", "")

	Ref: https://www.ibm.com/docs/en/db2/11.5?topic=db-connecting-database-server

GraphQL
=======
- resolver
	- It is a function that's responsible for populating the data for a single field in your schema.
	- It can populate that data in any way you define, such as by fetching data from a back-end
	  database or a third-party API.


Lazy Loading
	- it is the practice of delaying load or initialization of resources or objects until
	  they're actually needed to improve performance ans save system resources.
	- Advantages include,
		- Reduced initial load times
		- Bandwidth conservation
		- System resource conservation

Eager Loading
	- it is the preloading more than needed.


Neo4j vs mongoDB
================
- Primary database model is Graph DBMS for Neo4j; where as it is document store for MongoDB
- Unlike MongoDB, Neo4j has foreign keys
- MongoDB supports Map Reduce method, whereas Neo4j wil not
- MongoDB data is schema free; whereas schema is optional in Neo4j
- Neo4j doesnt support SQL queries; whereas MongoDB supports standard SQL queries to Read-Only SQL queries
  via the MongoDB Connector for BI
- Neo4j doesnt support Partitioning; whereas MongodB supports sharding partitionining methods.


SQL Alchemy Loading - https://docs.sqlalchemy.org/en/14/orm/loading_relationships.html

- Logical DB Design Tools
	1) Visual Paradigm ERD Tools
	2) Vertabelo
	3) Lucidchart
	4) SQL Server Database Modeler
	5) DeZign for Databases
	6) Erwin Data Modeler
	7) Aqua Data Studio ER Modeler
	8) DbWrench
	9) IBM InfoSphere Data Architect
	10) DbDesigner.net
	11) dModelAid
	12) dbdiagram.io
	13) Diagrams.net (formerly Draw.io)
	14) QuickDBD
	15) ERD Plus

- Data Warehouse
	- https://aws.amazon.com/data-warehouse/
	- single, complete and consistent store of data obtained from
	  a variety of different sources made available to end users
	- TYPE
		1) OLTP -- for warehouse
			- Involve the day-to-day processing of transactions, such as data ingestion, updates, and real-time data retrieval.
			- optimized for transactional throughput and concurrency
			- designed to handle frequent insert, update, and delete operations on smaller subsets of data
			- prioritize data consistency and integrity and support high transactional throughput with low latency.

		2) OLAP -- for application
			- Involve complex analytical queries and multidimensional analysis.
			- focuses on providing users with ability to analyze data from different perspectives or dimensions
			- designed to handle large volumes of historical data and support sophisticated calculations and aggregations.
			- provide features such as drill-down, slice-and-dice, pivot, and advanced visualization capabilities to help users gain insights from the data.
			- OLAP is optimized for query performance and ad hoc analysis.

			- By data storage mode, OLAP is three types:
				1) MOLAP is a multi-dimensional storage mode
				2) ROLAP is a relational mode of storage
				3) HOLAP is a combination of multi-dimensional and relational elements.

- Fact Table
	- A fact table is a primary table in a dimensional model.
	- A Fact Table contains
		1) Measurements/facts
		2) Foreign key to dimension table


- Dimension Table
	- A dimension table contains dimensions of a fact.
	- They are joined to fact table via a foreign key.
	- Dimension tables are de-normalized tables.
	- The Dimension Attributes are the various columns in a dimension table
	- Dimensions offers descriptive characteristics of the facts with the help of their attributes
	- No set limit set for given for number of dimensions
	- The dimension can also contain one or more hierarchical relationships

- Fact table vs Dimension table
	- Fact table contains measurements, metrics, and facts about a business process, while the
	  Dimension table is a companion to the fact table, which contains descriptive attributes to be used as query constraining.
	- Fact table is located at the center of a star or snowflake schema, whereas the
	  Dimension table is located at the edges of the star or snowflake schema.
	- Fact table is defined by its grain or most atomic level, whereas a Dimension table
	  should be wordy, descriptive, complete, and of assured quality.
	- Fact table helps to store report labels, whereas Dimension table contains detailed data.
	- Fact table does not contain a hierarchy, whereas the Dimension table contains hierarchies.

- Fact Table Types
	1) Additive		->  Measures should be added to all dimensions.
	2) Semi-Additive->	In this type of facts, measures may be added to some dimensions and not with others.
	3) Non-Additive	->	It stores some basic unit of measurement of a business process.
	                    Some real-world examples include sales, phone calls, and orders.

- Dimension Table Types
	1) Conformed Dimensions
		- Conformed dimensions is the very fact to which it relates.
		- This dimension is used in more than one-star schema or Datamart.
	2) Outrigger Dimensions
		- A dimension may have a reference to another dimension table.
		- These secondary dimensions called outrigger dimensions.
		- This kind of Dimensions should be used carefully.
	3) Shrunken Rollup Dimensions
		- Shrunken Rollup dimensions are a subdivision of rows and columns of a base dimension.
		- These kinds of dimensions are useful for developing aggregated fact tables.
	4) Dimension-to-Dimension Table Joins
		- Dimensions may have references to other dimensions.
		- However, these relationships can be modeled with outrigger dimensions.
	5) Role-Playing Dimensions
		- A single physical dimension helps to reference multiple times in a fact table as each reference linking to a logically distinct role for the dimension.
	6) Junk Dimensions
		- It a collection of random transactional codes, flags or text attributes.
		- It may not logically belong to any specific dimension.
	7) Degenerate Dimensions
		- Degenerate dimension is without corresponding dimension. It is used in the transaction and collecting snapshot fact tables.
		- This kind of dimension does not have its dimension as it is derived from the fact table.
	8) Swappable Dimensions
		- They are used when the same fact table is paired with different versions of the same dimension.
	9) Step Dimensions
		- Sequential processes, like web page events, mostly have a separate row in a fact table for every step in a process. It tells where the specific step should be used in the overall session.


- star schema (or) dimension modelling
	- fact tables are used to record a business event and dimension tables are used to record the attributes of business items(eg user, item tables in an e-commerce app).
	- For example, in an e-commerce website, a fact table would contain information about orders, such as when the order was placed, the items in that order, who placed that order, etc.
	The dimension tables would be an item table (containing item id, item price, size, etc) and an user table (containing user id, user name, user address etc).

- slowly changing dimensions
	- It is a dimension that stores and manages both current and historical data over time, in a data warehouse.
	- It helps in tracking the history of dimension records.
	- Example of such dimensions could be: customer, geography, employee.
	- Types
		- Type 0 SCD – The Fixed(or passive) Method.
		- Type 1 SCD – Overwriting the old value by new values.
		- Type 2 SCD – Creating a new additional record by row versioning.
						means current record is marked as inactive, and latest record is marked open/active
		- Type 3 SCD – Adding a new column to show the previous value.
						means new column will store the previous existing value, for that column, in the record.
		- Type 4 SCD – Using historical table.
						means, the dimension table has the latest value while its history is maintained in a separate table.
		- Type 6 SCD - A combination of Type 1, 2 & 3 are used to track changes in dimension.
		              Type 6 is adopted in scenarios where multiple parts of a record are slowly changing dimensions, but using multiple implementations of a single type could lead to issues with rapid inflation of table size.

	- In detail,
		Type 0
			- The passive method.
			- In this method no special action is performed upon dimensional changes.
			- Some dimension data can remain the same as it was first time inserted, others may
			  be overwritten.
		Type 1
			- Overwriting the old value.
			- In this method no history of dimension changes is kept in the database.
			- The old dimension value is simply overwritten be the new one.
			- This type is easy to maintain and is often use for data which changes are caused
			  by processing corrections(e.g. removal special characters, correcting spelling errors).
		Type 2
			- Creating a new additional record.
			- In this methodology all history of dimension changes is kept in the database.
			- You capture attribute change by adding a new row with a new surrogate key to the
			  dimension table.
			- Both the prior and new rows contain as attributes the natural key(or other durable
			  identifier).
			- Also 'effective date' and 'current indicator' columns are used in this method.
			- There could be only one record with current indicator set to 'Y'.
			- For 'effective date' columns, i.e. start_date and end_date, the end_date for current
			  record usually is set to value 9999-12-31.
			- Introducing changes to the dimensional model in type 2 could be very expensive
			  database operation so it is not recommended to use it in dimensions where a new
			  attribute could be added in the future.
		Type 3
			- Adding a new column.
			- In this type usually only the current and previous value of dimension is kept in
			  the database.
			- The new value is loaded into 'current/new' column and the old one into 'old/previous'
			  column.
			- Generally speaking the history is limited to the number of column created for
			  storing historical data.
			- This is the least commonly needed technique.
		Type 4
			- Using historical table.
			- In this method a separate historical table is used to track all dimension's
			  attribute historical changes for each of the dimension.
			- The 'main' dimension table keeps only the current data e.g. customer and
			  customer_history tables.
		Type 6
			- Combine approaches of types 1,2,3 (1+2+3=6).
			- In this type we have in dimension table such additional columns as:

			current_type 	- for keeping current value of the attribute. All history records
							  for given item of attribute have the same current value.
			historical_type - for keeping historical value of the attribute. All history
							  records for given item of attribute could have different values.
			start_date 		- for keeping start date of 'effective date' of attribute's history.
			end_date 		- for keeping end date of 'effective date' of attribute's history.
			current_flag 	- for keeping information about the most recent record.

			- In this method to capture attribute change we add a new record as in type 2.
			- The current_type information is overwritten with the new one as in type 1.
			- We store the history in a historical_column as in type 3.


- SCD vs CDC ( sowly changing dimensions vs change data capture)
	- SCD is ideal for organizations that must maintain a record of all changes to the data flowing through their systems.
	- And CDC is ideal if the business process requires only that the changed data arrive in to the target.


- star schema vs snowflake schema
	- Snowflake Schema is an extension of a Star Schema, and it adds additional dimensions.
	- In a star schema, only single join defines the relationship between the fact table and any dimension tables.
	  whereas Star schema contains a fact table surrounded by dimension tables.
	- Snowflake schema is surrounded by dimension table which are in turn surrounded by dimension table.
	- A snowflake schema requires many joins to fetch the data.
	- Comparing Star vs Snowflake schema, Start schema has simple DB design, while Snowflake schema has very complex DB design.

	- https://www.guru99.com/star-snowflake-data-warehousing.html

DataMart vs DataWarehouse Vs DataBase vs Operational DataStore
	- https://www.zuar.com/blog/data-mart-vs-data-warehouse-vs-database-vs-data-lake/

Data warehouse vs DataMart
	- data warehouses are built to serve as the central store of data for the entire business,
	- data mart fulfills the request of a specific division or business function.

	- In terms of range,
		- a data mart is limited to a single focus for one line of business;
		- a data warehouse is typically enterprise-wide and ranges across multiple areas.
	- And, in terms of sources,
		- a data mart includes data from just a few sources;
		- a data warehouse stores data from multiple sources.

Q) How to get last day of previous month?

	SELECT LAST_DAY(CURDATE());						-- Current month
	SELECT LAST_DAY(CURDATE() - INTERVAL 1 MONTH);	-- Previous month
	SELECT LAST_DAY(CURDATE() + INTERVAL 1 MONTH);	-- Next month


DB Design Books
===============
	SRE book, https://sre.google/sre-book/table-of-contents/
	Designing Data-Intensive Applications (O’Reilly)
	Streaming Systems (O’Reilly)
	Clean Code


CAP Theorem
===========
	Consistency        : Every read receives the most recent write or an error
	Availability       : Every request receives a (non-error) response, without the guarantee that it contains the most recent write
	Partition tolerance: The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes

- When a network partition failure happens, we can choose to
	- Cancel the operation and thus decrease the availability but ensure consistency, or
	- Proceed with the operation and thus provide availability but risk inconsistency

- It is impossible for distributed systems to guarantee consistency, availability and partition tolerance at same time.
- Apache cassandra is an AP system;
	- means availability and partition tolerance holds true,but not for Consistency
	- but this can further tuned via replication factor(how many copies of data) and consistency level (read and write).

DB Partitioning
===============
- Partitioning is the db process where very large tables are divided into multiple smaller parts.
- It is done mainly for manageability, performance or availability reasons, or for load balancing
- This helps in maintenance of large tables and to reduce overall response time to read/load data for particular SQL operations.
- popular in distributed database management systems, where each partition may be spread over multiple nodes, with users at the node performing local transactions on the partition.
- They are of TWO types
	1) Vertical DB partitioning
		- Vertical partitioning splits a table into two or more tables containing different columns
		- To reduce access times, wide text or BLOB columns can be split to its own table.
		- Another example is to restrict access to sensitive data e.g. passwords, salary information etc.
	2) Horizontal partitioning involves putting different rows into different tables.
		- A view with a union of these partitions, will provide complete view of all customers.


TODO
	https://learnsql.com/

	• Intro to SQL 			➤ https://lnkd.in/eq6_-KqX
	• SQL Tutorial 			➤ https://lnkd.in/eSZAFVrJ
	• SQL for Data Analysis ➤ https://lnkd.in/e6RMcn7g
	• SQL Notes 			➤ https://lnkd.in/ejy_tqHz

	https://learnsql.com/blog/

	https://www.sqlshack.com/sql-lag-function-overview-and-examples/

	http://sqlfiddle.com/
	https://sqltest.net/
	https://learn.microsoft.com/en-us/sql/t-sql/functions/functions?view=sql-server-ver16
	https://datalemur.com/

	https://dbfiddle.uk/
	https://database.guide/
	https://www.databasestar.com/sql-cheat-sheets/
	https://www.stratascratch.com/

	https://github.com/Coder-World04/Complete-Advanced-SQL-Series

	https://dev.mysql.com/doc/refman/8.0/en/tutorial.html
	https://www.linkedin.com/events/sqlfordataanalytics-datascience7040893287821426688/comments/

	https://www.databasestar.com/learn-sql/
	https://www.kaggle.com/learn/intro-to-sql

	1. SQLBolt		http://sqlbolt.com
	2. SQLZoo		https://sqlzoo.net/wiki/SQL_Tutorial
	3. SQLTest		http://sqltest.net
	4. W3Schools	http://w3schools.com/sql/
	5. Codecademy	http://codecademy.com/learn/learn-sql


	☑️ Python 						➟ hackerrank .com
	☑️ Data Structures & Algorithms ➟ Leetcode .com
	☑️ Data Visualization 			➟ jovian. ml
	☑️ Machine Learning 			➟ Kaggle .com
	☑️ Mathematics 					➟ desmos .com
	☑️ Statistics 					➟ seeing-theory.brown.edu
	☑️ BigQuery 					➟ cloudskillsboost.google/quests/123

	📈 For Visualization: Storytelling with Data by Cole Nussbaumer Knaflic.
	📉For SQL: Learning SQL by Alan Beaulieu.
