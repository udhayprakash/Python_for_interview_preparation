cloud computing services:
1)Infrastructure-as-a-Service (IaaS),
2)Platforms-as-a-Service (PaaS), and
3)Software-as-a-Service (SaaS)

AWS global vs Regional services
===============================
- AWS global services:
	Amazon S3 (Simple Storage Service)
	Amazon DynamoDB (NoSQL database)
	Amazon EC2 (Elastic Compute Cloud)
	Amazon VPC (Virtual Private Cloud)
	Amazon Route 53 (DNS service)
	AWS IAM (Identity and Access Management)
	AWS CloudFront (Content Delivery Network)
	AWS CloudTrail (Auditing and Compliance)
	AWS Config (Configuration Management)
	AWS CloudFormation (Infrastructure as Code)

- AWS regional services:
	Amazon RDS (Relational Database Service)
	Amazon Elastic Beanstalk (Application Platform as a Service)
	Amazon WorkSpaces (Desktop-as-a-Service)
	AWS Lambda (Serverless computing)
	Amazon Redshift (Data Warehousing)
	Amazon Elasticache (In-memory caching)
	Amazon EMR (Elastic MapReduce)
	Amazon Kinesis (Real-time data processing)
	AWS Glue (ETL service)
	AWS Snowball (Data transfer service)

EC2 Instance
============
Reserved Instances
	– Reduce your Amazon EC2 costs by making a commitment to a consistent
	instance configuration,including instance type and Region, for a term
	of 1 or 3 years.
Spot Instances
	– Request unused EC2 instances, which can reduce your Amazon EC2
	costs significantly.
	- Types
		- Regular spot pricing—instances can be terminated with 2 minutes notice.
		- Defined duration—you can get a spot instance guaranteed to run for a
		  period of 1-6 hours.
		  The longer the defined duration, the lower the discount provided for the spot instance.

ec2 types
1) General Purpose
2) Compute Optimized
3) Memory Optimized
4) Accelerated Computing
5) Storage Optimized
6) Instance Features

boto3
=====
- boto3 is built on top of botocore
	- client is for lower-level interactions
	- resource for higher-level object oriented abstractions
	- waiters are polling the status of specific resource
		- say, when creating ec2 instance, waiting till it reaches "Running" state
	- collections indicate a group of resources such as a group of s3 objects
	  in a bucket, or a group of SQS queues
		- It helps to perform some action on those groups

	- upload file to s3 bucket
		import boto3
		s3 = boto3.resource('s3')
		s3.meta.client.upload_file('/tmp/hello.txt', 'mybucket', 'hello.txt')

ec2 vs Lambda
=============
- Lambda is suitable for event-driven programming, whereas
	EC2 is suitable for more tailored solutions
- AWS EC2 is sutable for
	- hosting web sites
	- developing and testing applications or complex environments
	- high performance computing
	- disaster recovery
- And, Lambda is suitable for
	- Automating tasks
	- Processing objects uploaded to Amazon S3
	- Real-time log analyzing
	- Real-time filtering and transforming data


Lambda
=======
IN AWS Lambda,
1) I  developed Lambda functions to process data streams from Kinesis, transforming and loading data into S3 or DynamoDB.
2) Also, Creating API endpoints using Lambda and API Gateway, often for data retrieval or updates.
3) And, worked on creating event-triggered jobs, by configuring s3 bucket events like new file creation, file update/deletion, etc.
4) Also, configured lambda to run with AWS Event bridge, based events too, and time-triggered jobs, with Event Scheduler
5) And, worked with Step functions, for integrating multiple jobs.
6) Optimizing Lambda functions for performance and cost, including managing dependencies, minimizing cold starts, and efficient memory allocation.
7) Implementing scheduled tasks using Lambda and CloudWatch Events for regular data updates or maintenance jobs.
8) Integrating Lambda with other AWS services like SQS, SNS, and Step Functions for complex workflows.

9) Within lambdas, I used boto3 to connect with any AWS resource;
   and AWS RDS proxy for RDS database integrations.

- Lambda Triggers
	API Gateway
	Application Load Balancer
	CloudFront
	CloudWatch Events
	CloudWatch Logs
	DynamoDB
	Kinesis
	S3
	SNS
	SQS
	AWS IoT
	Alexa Skills Kit
	Alexa Smart Home
	CodeCommit
	Cognito Sync Trigger

	- scheduled trigger with rate() function, we can trigger lambda, for every 5min, 1min, etc

- Lambda layer
	- It is an archive containing additional code, such as libraries, dependencies, or even custom runtimes.
	- When you include a layer in a function, the contents are extracted to the /opt directory in the execution environment.

	- For a Lambda function, there is a maximum of 5 layers and a maximum size for all layers of 250 MB (uncompressed).
	  This maximum applies regardless of whether you are using an official AWS runtime or a custom runtime.
	  The Application Security layer size is about 30 MB.

- chalice framework
	- local development setup for working with AWS Lambdas

- AWS Lambda Performance Optimizaton
	- increasing RAM => faster execution => same price.
	- Watch out for function size to reduce the cold start durations.
	- Split complex processes into separate functions to save money and gain speed.
	- When possible, execute code in parallel.
	- Reusing connections with Keep-Alive.

- Lambda Cold Start
	- https://www.simform.com/blog/lambda-cold-starts/
	- Lambda has better latency per request than EC2, but for first request, due to cold start,
	it is significantly slower.
	- To reduce this cold start, Earlier, i used to fire a dumy request periodically.
	- Also, we can assign provisioned concurrency, which means keeping a certain number of
	function containers ready for invocations.

- AWS Lambda Advantages
	- Reduced Cost of Execution
	- Improved Application Resiliency

- AWS Lambda Limitations
	- The maximum execution timeout for a function is 15 minutes*.
	- The disk space (ephemeral) is limited to 512 MB.
	- The default deployment package size is 50 MB.
	- The memory range is from 128 to 3008 MB.
	- Requests limitations by Lambda:
		- Request and response (synchronous calls) body payload size can be up to to 6 MB.
		- Event request (asynchronous calls) body can be up to 128 KB.
	- More Complex Call Patterns
	- No Control Over Environment

- AWS Lambda event source mappings
	- An event source mapping is a Lambda resource that reads from an event source and
	invokes a Lambda function.
	- we can use event source mappings to process items from a stream or queue in services
	that don't invoke Lambda functions directly.
	- Lambda provides event source mappings for
		- Amazon DynamoDB
		- Amazon Kinesis
		- Amazon MQ
		- Amazon Managed Streaming for Apache Kafka (Amazon MSK)
		- Self-managed Apache Kafka

- AWS Lambda Examples
	- say, for Returning a message

	def lambda_handler(event, context):
		message = 'Hello {} {}!'.format(event['first_name'], event['last_name'])
		return {
			'message' : message
		}

	- The first argument is the event object.
	  An event is a JSON-formatted document that contains data for a Lambda function
	  to process. The Lambda runtime converts the event to an object and passes it to
	  your function code. It is usually of the Python dict type. It can also be list,
	  str, int, float, or the NoneType type.
	- The event object contains information from the invoking service.
	  When you invoke a function, you determine the structure and contents of the event.
	  When an AWS service invokes your function, the service defines the event structure.
	- The second argument is the context object.
	  A context object is passed to your function by Lambda at runtime.
	  This object provides methods and properties that provide information about the
	  invocation, function, and runtime environment.

	- AWS lambda can be invoked both synchronously or asynchronously
	  In synchronous invocation, lambda runs the function and waits for response.
		 when function completes, lambda returns response from the function's code with
		 additional data, such as the version of the function that was invoked.

		AWS CLI command for the same:
			aws lambda invoke --function-name my-function --payload '{ "key": "value" }' response.json

	  In asynchronous invocation, we dont wait for function response.
		you handoff event to lambda and handles rest.
		we configure how Lambda handles errors, and can send invocation records to a downstream
		resource to chain together components of your application.
		AWS services like S3 and SNS invoke functions asynchronously to process the events.
		Error handling for asynchronous events:
		we can configure
			1) Maximum age of event – The maximum amount of time Lambda retains an event
							 in the asynchronous event queue, up to 6 hours.
			2) Retry attempts – The number of times Lambda retries when the function
							 returns an error, between 0 and 2

		AWS CLI command:
			aws lambda invoke --function-name my-function  \
				  --invocation-type Event --cli-binary-format raw-in-base64-out \
						  --payload '{ "key": "value" }' response.json


AWS Lambda Context properties

	function_name 			– The name of the Lambda function.
	function_version 		– The version of the function.
	invoked_function_arn 	– The Amazon Resource Name (ARN) that's used to invoke the function.
								Indicates if the invoker specified a version number or alias.
	memory_limit_in_mb 		– The amount of memory that's allocated for the function.
	aws_request_id 			– The identifier of the invocation request.
	log_group_name 			– The log group for the function.
	log_stream_name 		– The log stream for the function instance.

	identity – (mobile apps) Information about the Amazon Cognito identity that authorized the request.
		cognito_identity_id 	– The authenticated Amazon Cognito identity.
		cognito_identity_pool_id– The Amazon Cognito identity pool that authorized the invocation.

	client_context – (mobile apps) Client context that's provided to Lambda by the client application.
		client.installation_id
		client.app_title
		client.app_version_name
		client.app_version_code
		client.app_package_name
		custom – A dict of custom values set by the mobile client application.
		env – A dict of environment information provided by the AWS SDK.

- AWS Lambda vs ECS
	- ECS is best used for running a Docker environment on AWS using clustered instances.
	- Lambda is best used for quickly deploying small, on-demand applications in a serverless environment

- AWS Lambda Vs Glue
	- Lambda can use a number of different languages (Node.js, Python, Go, Java, etc.)
	  whereas Glue can only execute jobs using Scala or Python code.
	- Lambda can execute code from triggers by other services (SQS, Kafka, DynamoDB,
	  Kinesis, CloudWatch, etc.) vs. Glue which can be triggered by lambda events,
	  another Glue jobs, manually or from a schedule.
	- Lambda runs much faster for smaller tasks vs. Glue jobs which take longer to
	  initialize due to the fact that it's using distributed processing. That being
	  said, Glue leverages its parallel processing to run large workloads faster than Lambda.
	- Lambda looks to require more complexity/code to integrate into data sources
	  (Redshift, RDS, S3, DBs running on ECS instances, DynamoDB, etc.) while Glue
	  can easily integrate with these. However, with the addition of Step Functions,
	  multiple lambda functions can be written and ordered sequentially due reduce
	  complexity and improve modularity where each function could integrate into a
	  aws service (Redshift, RDS, S3, DBs running on ECS instances, DynamoDB, etc.)
	- Glue looks to have a number of additional components, such as Data Catalog which
	  is a central metadata repository to view your data, a flexible scheduler that
	  handles dependency resolution/job monitoring/retries, AWS Glue DataBrew for
	  cleaning and normalizing data with a visual interface, AWS Glue Elastic Views
	  for combining and replicating data across multiple data stores, AWS Glue Schema
	  Registry to validate streaming data schema.

- AWS Glue
	- Its a serverless Data integration ETL tool.
	- We can do all data integraton works, starting from
		- data discovery
		- data preparation and EDA - Exploratory data analysis
		- ETLs,
		- cleansing
		- transforming
		- and finally, centrally cataloging.

	- data flow
		Data sources -> Crawler	-> Data Catalog
		1) Crawlers run classifiers ( either builtin, or custom classifiers)
		2) connects to the data sources, and infer the data schemas
		3) Extracts meta data and store in Data Catalog.

	- AWS Glue DataBrew
		- Its a visual data preparation tool to manually clean and normalize data, without writing code.
		- There were many ready-made transformations to automate data preparation tasks, such as
			- filtering anomalies,
			- converting data to standard formats, and
			- correcting invalid values, etc.


	- Data catalog
		- Each Data Catalog is a highly scalable collection of tables organized into databases.
		- Its one place to store and find metadata to keep track of data schema changes.
		- Each AWS account has one AWS Glue Data Catalog per AWS Region.
		- A table is metadata representation of a collection of structured or semi-structured data stored in sources such as Amazon RDS, etc.

	- https://docs.aws.amazon.com/glue/latest/dg/add-job.html
	- https://docs.aws.amazon.com/glue/latest/dg/workflows_overview.html
	- AWS Glue connectors
		- Amazon S3
		- JDBC
			- Amazon Redshift
			- Amazon Relational Database Service (Amazon RDS)
		- Amazon DocumentDB
		- DynamoDB
		- Kafka
		- Amazon Kinesis
		- MongoDB
		- Network (designates a connection to a data source within an
					Amazon Virtual Private Cloud environment (Amazon VPC))

	- Glue Limitations
		- AWS Glue cannot support the conventional relational database systems. It can only support structured databases.
		- AWS Glue is a managed ETL service for Apache Spark. And it is not a full-fledged ETL service like Talend, Xplexty, etc.
		  So, it will take more work for customizations
		- AWS Glue requires you to test the changes in the live environment. It does not provide the test environment to analyze the repercussions of a change.
		- glue jobs will take long time for
			- large datasets
			- Non-uniform distribution of data in the datasets
			- Uneven distribution of tasks across the executors
			- Resource under-provisioning
			- solution:
				we should enable metrics and continous logging;
				Also, troubleshooting using spark UI

- ETL Jobs
	- I have created ETLs jobs like say,
		Users can connect to an endpoint, from where request goes via
			AWS Cognito for authentication , Then
			AWS API Gateway and then
			aws lambda and from there
			it can make AWS Athena queries on top of s3 buckets
			and reply that response in JSON

- RDS Proxy
	-RDS Proxy acts as an intermediary between your application and an RDS database.
	- RDS Proxy establishes and manages the necessary connection pools to your database
	  so that your application creates fewer database connections.
	- It connection pooling necessary for scaling many simultaneous connections
	  created by concurrent Lambda functions.


- AWS Redshift
	- Materialized View
		- It is similar to the db views for RDBMS
		- useful for speeding up queries that are predictable and repeated.
		- A materialized view contains a precomputed result set, based on an SQL query
		over one or more base tables.
		- It returns the precomputed results from the materialized view, without having
		to access the base tables at all.
	- Load CSV to Redshift
		1) Create the schema on Amazon Redshift.
		2) Load the CSV file to Amazon S3 bucket using AWS CLI or the web console.
		3) Import the CSV file to Redshift using the COPY command.
		4) Generate AWS Access and Secret Key in order to use the COPY command.
	  - Also, we can autoload using a 3rd party tool, called Skyvia

AWS DynamoDB
	is a fully managed proprietary NoSQL database
	service that provides fast and predictable performance with seamless scalability.
	It supports key-value and document data structures
	DynamoDB allows users to create tables for your database to store
	and retrieve any data volume and to support any request traffic level.

	LSI - allows you to perform a query on a single Hash-Key while using multiple
			different attributes to "filter" or restrict the query.
	GSI - allows you to perform queries on multiple Hash-Keys in a table, but costs
			extra in throughput, as a result.

	DynamoDB Streams is a powerful service that you can combine with other AWS services
	to solve many similar issues.
	When you enable DynamoDB Streams, it captures a time-ordered sequence of item-level
	modifications in a DynamoDB table and durably stores the information for up to 24 hours.

	projection Expressions
		- When we use GetItem, Query or Scan, we get data with all item attributes, by default.
		- To get only some specific attributes, we use projection expression.
		- It is a string that identifies the attributes we want.

AWS Athena
	- Its a interactive query service for querying on S3 using standard SQL
	- Performance Tuning Amazon Athena
		1. Partition the data
			- means, dividing the table into parts and keeping the related data together based on columns value like date, country, region, etc.
			- As partitions act as virtual columns, it helps reduce the amount of data scanned per query, thereby improving performance.
			- Also, we can specify filters on these patitions, similar to on columns, to refine the queries.

		2. Bucket the data
			- means, to bucket the data within a single partition. With bucketing,
			we can specify one or more columns containing rows that we want to group together,
			and put those rows into multiple buckets.
			- This helps to query only the bucket that we need to read when the bucketed columns value is specified,
			which can dramatically reduce the number of rows of data to read, which in turn reduces the cost of running the query.

		3. Use compression
			- using either BZIP2, DEFLATE, Gzip, LZ4, LZO, Snappy, Zlib, Zstd, etc for the compression
			- compression the files reduces the file sizes, which reduces the data scanned from S3,
			  resulting in lower cost of running queries. It , inturn, reduces the network traffic between Amazon s3 to Athena.

		4. Optimize file size
		5. Optimize columnar data store generation

Amazon Athena Workgroups are resource types, used to separate query execution and query history between Users, Teams, or Applications running under the same AWS account.
Because Workgroups act as resources, you can use resource-based policies to control access to a Workgroup.

I have experience migrating applications and data, from on-prem to cloud

- AutoScaling
--------------
- Autoscaling helps in scaling up or down the resources, to adjust to fluctuating traffic or demand.
- Autoscaling can be done both for
	- EC2 instances, based on CPU load
	- Aurora databases, based on CPU utilization on Aurora replicas

- Advantages
	- Improved performance, Reduced Costs and Improved Reliability.
- Limitations
	a) Complexity: Autoscaling can be complex to implement and manage.
	b) Overprovisioning: Autoscaling can lead to overprovisioning of resources, which can increase costs.
	c) Downtime: Autoscaling can cause downtime during scaling events, which can impact your application's availability.

- autoscaling can be done
	1) Manually, in the console
	2) Dynamic Scaling
		- Dynamic scaling scales the capacity of Auto Scaling group as traffic changes occur.
		- dynamic scaling is of THREE types:
			a) Target Tracking scaling
				- Increase and decrease the current capacity of the group based on a Amazon CloudWatch metric and a target value.
			b) Step Scaling
				- Increase and decrease the current capacity of the group based on a set of scaling adjustments, known as step adjustments, that vary based on the size of the alarm breach.
			c) Simple Scaling
				- Increase and decrease the current capacity of the group based on a single scaling adjustment, with a cooldown period between each scaling activity.

	3) Predictive Scaling
		- scaling, in advance, based on daily and weekly patterns in traffic flows.
		- Mostly predictive scaling is suitable when we have
			- Cyclical traffic, such as high use of resources during regular business hours and low use of resources during evenings and weekends
			- Recurring on-and-off workload patterns, such as batch processing, testing, or periodic data analysis
			- Applications that take a long time to initialize, causing a noticeable latency impact on application performance during scale-out events

	4) Scheduled Scaling
		- Scaling based on the created scheduled actions, for specific date and timestamp.


- IAM
-------
	- User - indivdual
	- Role -> service to sevice relation
	- Group --> For more than one resource
	- policy --> basic permission entity

	- IAM roles define the set of permissions for making AWS service request
	- IAM policies define the permissions that you will require.

	- Admin or root user for the account can create IAM identities.
	- An IAM identity provides access to an AWS account.
	- user group is a collection of IAM users managed as a unit.
	- IAM identity represents a user, and can be authenticated and then authorized to
	  perform actions in AWS. Each IAM identity can be associated with one or more policies.

	- IAM groups have multiple users, under it; but multiple groups CANT be under another group.
	- Policies determine what actions a user, role, or member of a user group can perform,
	  on which AWS resources, and under what conditions.
		- Identity-based policies are permissions policies that you attach to an IAM identity, such as an IAM user, group, or role.
		- Resource-based policies are permissions policies that you attach to a resource such as an Amazon S3 bucket or an IAM role trust policy.

	- Identity-based policies are attached to an IAM user, group, or role.
		- Identity-based policies can be managed or inline.
	- Resource-based policies are attached to a resource.
		- We can attach resource-based policies to
			Amazon S3 buckets,
			Amazon SQS queues,
			VPC endpoints, and
			AWS Key Management Service encryption keys.
		- With resource-based policies, we can specify who has access to the resource and what actions they can perform on it.
	- Resource-based policies Vs resource-level permissions
		- We can attach resource-based policies directly to a resource
		- Resource-level permissions refer to ability to use ARNs to specify individual resources in a policy.

	Identify Based Policies
		Effect 	– whether to allow or deny the action(s)
		Action 	– the API(s) the policy applied to requests on resources (‘*’ means wildcard)
		Resource– Identifier of the resource(s) to which the policy applies.
		          Resources are identified by an Amazon Resource Name (ARN), or by wildcard.

			{
				"Version": "2012-10-17",
				"Statement": [
					{
						"Effect": "Allow",
						"Action": [
							"s3:Get*",
							"s3:List*"
						],
						"Resource": "*"
					}
				]
			}
	Resource-based Policies
		Resource-based policies grant permissions to the principal that is specified in the policy.
		They specify who or what can invoke an API from a resource to which the policy is attached.
		{
		  "Version": "2012-10-17",
		  "Id": "default",
		  "Statement": [
			{
			  "Sid": "s3-event-cezary_for_lambda-s3",
			  "Effect": "Allow",
			  "Principal": {
				"Service": "s3.amazonaws.com"
			  },
			  "Action": "lambda:InvokeFunction",
			  "Resource": "arn:aws:lambda:eu-west-1:1234567890:function:lambda-s3",
			  "Condition": {
				"StringEquals": {
				  "AWS:SourceAccount": "1234567890:"
				},
				"ArnLike": {
				  "AWS:SourceArn": "arn:aws:s3:::test-bucket-cezary"
				}
			  }
			}
		  ]
		}

- s3 policy vs IAM policy
	- Bucket policies are similar to IAM user policies.
	- They're written in the same JSON syntax and can be used to provide granular permissions on S3 resources.
	- The main difference from IAM user policies is that bucket policies are attached to an S3 resource directly rather than to an IAM user.

- Tagging
	- SNS, SQS cant be tagged.
	- We can use tag editor to manage tags, but only to limited resources.

RDS vs DynamoDb
----------------
- Amazon RDS is relational, whereas DynamoDB is a NoSQL database engine.
- DynamoDB can support tables of any size, where as in RDS, the storage size changes
	based on the database engine used.
- Storing data in DyanmoDb costs more, compared to RDS or Aurora.
- DynamoDB is very fast as it is key-value database

- DynamoDB should not be used:
	- When multi-item or cross table transactions are required.
	- When complex queries and joins are required.
	- When real-time analytics on historic data is required.

SNS vs SQS
==========
SNS - sends messages to the subscriber using push mechanism and no need of pull.
SQS - it is a message queue service used by distributed applications to exchange messages through a polling model, and can be used to decouple sending and receiving components.

SQS offers a managed, highly scalable queue for buffering workload distribution across decoupled systems. SNS provides managed pub/sub capability for pushing instant notifications to multiple interested subscribers.

Use SQS when:
	You need a simple queue to buffer tasks and distribute workload across systems.
	Your systems need to poll and pull messages from the queue.
	Loose coupling and item-at-a-time processing is required.
	A distributed architecture with decoupled components is in place.
	Order of messages does not matter.
	You need a buffer for traffic spikes.

Use SNS when:
	You need push notifications to multiple subscribers and systems.
	Event notifications matter more than data payload.
	Fanout messaging is required to push to multiple consumers.
	Loose coupling is required without dependency on consumers.
	Instant push notifications are important, like alerts.
	Payload size is generally limited.

- sqs
	- Amazon SQS supports two types of queues:
		1) standard queues
			- The standard queue type provides high throughput, best-effort ordering, and at-least-once delivery of messages.
			- This means that messages can be delivered out of order and may be duplicated, but it ensures that every message will be delivered at least once.
		2) FIFO queues
			- The FIFO queue type provides both ordering and deduplication of messages.
			- It guarantees that messages are processed exactly once and in the order they are sent.
			- However, it has lower throughput compared to standard queues and higher costs.


- dead letter queue
	- A dead-letter queue is an Amazon SQS queue that an Amazon SNS subscription can
	target for messages that can't be delivered to subscribers successfully.
	- Messages that can't be delivered due to client errors or server errors are held
	in the dead-letter queue for further analysis or reprocessing.



AWS Limits
==========
	Lambda Environment variables limit is not per function; but overall @ no more than 4KB
		   disk space (ephemeral) is limited to 512 MB
		   default deployment package size is 50 MB
		   memory range is from 128 to 3008 MB
		   maximum execution timeout for a function is 15 minutes
		   3000 concurrent request at time. If exceeded, requsts will throttle until lambda scales by 500 per minute
	We can have max 255 gateway endpoints per VPC.


CloudFormation
===============
- Its Infrastructure as Code (IaC) service, written in either json or yaml format.
- We can manage all the resources, configurations, relation between then and persmission, in a code.

- CloudFormation Templates
	- A CloudFormation template consists of 6 sections
		– Description, for writing any project related comments describing it.
		– Parameters, for passing values to template at stack creation time, using "Ref" intrinsic function.
		– Mappings
		– Conditions
		– Resources and
		– Outputs.
	- Only the Resources section is required.
	- sample template
			{
				"AWSTemplateFormatVersion" : "version date",
				"Description" : "Valid JSON strings",

				"Parameters" : {
					set of parameters
				},

				"Mappings" : {
						set of mappings
						},

				"Conditions" : {
						set of conditions
						},

				"Resources" : {
					set of resources
				 },

				"Outputs" : {
					set of outputs
				}

			}

	- ref - https://www.xtivia.com/blog/introduction-aws-cloudformation-templates/

- CloudFormation parameters
	- Templates can be customized using parameters.
	- Each time we create or update the stack, parameters help us in giving template custom values at runtime.
- CloudFormation Mappings
	- Mapping enables us to map keys to a corresponding named value that we specify in a conditional parameter.

- intrinsic function
	- built-in functions that enable you to assign values to properties that are only available
	  at runtime.
	- we can use intrinsic functions in templates, to assign values to properties that are not
	  available until runtime.
	- Example
		Fn::Base64
		Fn::Cidr
		Condition functions
		Fn::FindInMap
		Fn::GetAtt
		Fn::GetAZs
		Fn::ImportValue
		Fn::Join
		Fn::Select
		Fn::Split
		Fn::Sub
		Fn::Transform

Stack Vs Stackset CloudFormation
- A stack set lets you create stacks in AWS accounts across AWS Regions by using a single AWS CloudFormation template.
  A stack set's CloudFormation template defines all the resources in each stack.
- A stack instance refers to a stack in a target account within an AWS Region and is associated with only one stack set.

- cloudformation limits
	The new per template limits for the maximum number of resources is 500 (previously 200), parameters is 200
	(previously 60), mappings is 200 (previously 100), and outputs is 200 (previously 60).
	CloudFormation allows you to model and provision cloud resources as code in a safe, predictable, and scalable manner.

boto3
------
- boto3 is built on top of botocore
	- client is for lower-level interactions
	- resource for higher-level object oriented abstractions
	- waiters are polling the status of specific resource
		- say, when creating ec2 instance, waiting till it reaches "Running" state
	- collections indicate a group of resources such as a group of s3 objects
	  in a bucket, or a group of SQS queues
		- It helps to perform some action on those groups

- List files in s3 bucket
	import boto3
	s3 = boto3.resource('s3')

	## Bucket to use
	bucket = s3.Bucket('my-bucket')

	## List objects within a given prefix
	for obj in bucket.objects.filter(Delimiter='/', Prefix='fruit/'):
		print(obj.key)

	Output:

		fruit/apple.txt
		fruit/banana.txt

AWS Secrets Manager
-------------------
	- Rotate secrets safely ( we can keep expiry and rotate values whenever needed )
	- Manage access with fine-grained policies ( we can create a policy that enables developers to retrieve secrets values )
	- Secure and audit secrets centrally ( it gives audit trail how many used from which account )
	- Pay as you go ( No of secret value and no of API calls made for retrieval )
	- Easily replicate secrets to multiple regions ( cross regions access is allow )

	- It enables to rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle.
	- secret rotation with built-in integration for Amazon Relational Database Service (Amazon RDS), Amazon Redshift,
	and Amazon DocumentDB.
	- Also, the service is extensible to other types of secrets, including API keys and OAuth tokens.
	- In addition, Secrets Manager enables you to control access to secrets using fine-grained permissions and audit secret rotation
	centrally for resources in the AWS Cloud, third-party services, and on-premises.

	- To access the secrets manager, we can use either
		- AWS credentials ( combination of access key and secret key )
		- AWS SDK ( server side SDK or client side SDK

s3 classification
-----------------
1) s3 standard
		- for frequently accessed data
2) s3 Intelligent-Tiering
		- for automatic cost savings for data with unknown or changing access patterns
3) s3 Standard-IA  (Infrequent Access)
		- for less frequently accessed data
4) s3 One Zone-IA
		- for less frequently accessed data
5) s3 Glacier Instant Retrieval
		- for archive data that needs immediate access
6) s3 Glacier Flexible Retrieval(formerly S3 Glacier)
		- for rarely accessed long-term data that does not require immediate access
7) s3 Glacier Deep Archive
		- for long-term archive and digital preservation with retrieval in hours at the lowest cost storage in the cloud
8) s3 Output
		- storage class to store your S3 data on premises, when we cant store data in AWS any compliance

Once an S3 Lifecycle policy is set, your data will automatically transfer to a different storage class without any changes to your application.

we can create life cycle rules to delete files from a specific folder, like older than 30 days, etc


- s3 Limitations
-----------------
	 - By default, each AWS account can create up to 100 buckets.
	 - But we can request to increase this bucket limit by contacting AWS support.
	 - But we can request to increase this bucket limit by contacting AWS support.
AWS account security tools
--------------------------

- IAM (Identity & Access Management)
	- Enables to create users and roles with permissions
	  too specific resources
	- Supports Multi-factor authentication & supports
	  single sign-on (SSO)

- Amazon GuardDuty
	- uses machine learning to look for malicious activity in AWS environment
	- combines cloudTrail event logs, VPC flow logs, S3 event logs
	  and DNS logs to continuouslt monitor and analyze all activity.
	- identifies issues like privilege escalation, exposed credentials, and
	  communication with malicious IP addresses and domains.
	- detects when Ec2 instances are serving malware or mining bitcoin

	- Also, detects access pattern anomalies such as API calls
	  in new regions, ...

- Amazon Macie
	- discovers and protects sensitive data stored in s3 buckets.
	- like personally-identifiable information, or personal health information,
	  through discovery jobs.
	- continously evaluates s3 buckets and alerts when bucket is
	  unencrypted, is publicly accessible, or is shared with aws accounts
	  outside the organization.

- AWS config
	- records and continuously evaluates your AWS resource configuration.
	- keeping a historical record of all changes to your resources, which is useful for compliance with legal requirements and your organization’s policies.
	- evaluates new and existing resources against rules that validate certain configurations.
	- Config is configured per region, so it’s essential to enable AWS Config in all regions to ensure all resources are recorded, including in regions where you don’t expect to create resources.

- AWS CloudTrail
	- tracks all activity in AWS environment.
	- It records all actions a user executes in the AWS console and all API calls as events.
	- You can view and search these events to identify unexpected or unusual requests in your AWS environment.
	- AWS CloudTrail Insights is an add-on to help identify unusual activity. It automatically analyzes your events and raises an event when it detects abnormal activity.
	- CloudTrail is enabled by default in all AWS accounts

- Security Hub
	- combines information from all the above services in a central, unified view. It collects data from all security services from multiple AWS accounts and regions, making it easier to get a complete view of your AWS security posture.


Application Security Tools
--------------------------
- Amazon Inspector
	- security assessment service for applications deployed on EC2.
	- assessments include network access, common vulnerabilities and exposures (CVEs), Center for Internet Security (CIS) benchmarks, and common best practices such as disabling root login for SSH and validating system directory permissions on your EC2 instances.
	- Based on agent application installed in EC2 VMs, Inspector
	  generates a report with a detailed list of security findings prioritized by severity.
	- Run Inspector as part of a gated check in deployment pipeline to assess your applications’ security before deploying to production.

- AWS Shield
	- fully-managed distributed denial-of-service (DDoS) protection service.
	- enabled by default as a free standard service with protection against common DDoS attacks against your AWS environment.

- AWS Web Application Firewall (WAF)
	- monitors and protects applications and APIs built on services such as CloudFront, API Gateway, and AppSync.
	- can block access to your endpoints based on different criteria such as the source IP address, the request’s origin country, values in headers and bodies, and more (i.e, you can enable rate limiting, only allowing a certain number of requests per IP

- AWS Secrets Manager
	- managed service where you can store and retrieve sensitive information such as database credentials, certificates, and tokens.
	- Use fine-grained permissions to specify exact actions an entity can perform on the secrets, such as creating, updating, deleting, or retrieving secrets.
	- Secrets Manager also supports automatic rotation for AWS services such as Amazon Relational Database Service (RDS).


start/stop ec2 using cloudwatch logs
=====================================
- I set an CloudWatch alarm and once it is triggered, it will notify an SNS topic.
- Then, this SNS topic will invoke a Lambda function, which can then start your EC2 instance.
- Then, Created an AWS Lambda function that starts the EC2 instance.
- And, configured the SNS topic to invoke your Lambda function when it receives messages.

AWS Local Zones
===============
- a new type of infrastructure that places compute, storage, database and other services closer to end users and population centers so you can run applications that require single-digit mili second latency and meet data residency requirements, when an AWS Region is not close enough.


ALB vs ELB
===========
- ALB means application Load Balancer, whereas ELB means Elastic Load balancer
- ALB is layer 7 whereas ELB is layer 4
	- ALBs can route traffic based on the content of the request, while ELBs can only route traffic based on the IP address or port number of the request.

- ALB supports AWS lambda integration, whereas ELB doesnt support
	- means that ALBs can be used to offload some of the processing of requests to Lambda functions, which can improve the performance of your application.

- ALBs support HTTP/2 and WebSockets, while ELBs only support HTTP/1.1.
	- means that ALBs can handle more concurrent connections and can provide a better user experience for applications that use HTTP/2 or WebSockets.

- ALBs are more expensive than ELBs.
	- because ALBs offer more features and capabilities than ELBs

- Sticky sessions (or)  session affinity
========================================
- It is a load balancing technique that ensures that all requests from a single user are routed to the same server.
- This is done by storing a unique identifier for the user in a cookie, which is then sent with each request.
- The load balancer can then use this identifier to route the request to the correct server.
- Advantages
	1) Improved performance: Sticky sessions can improve the performance of web applications by reducing the number of times that users have to send session information to the server.
	2) Improved user experience: Sticky sessions can improve the user experience by ensuring that users are always routed to the same server, which can reduce latency and improve reliability.
	3) Reduced load on servers: Sticky sessions can reduce the load on servers by ensuring that each server only handles requests from a single user.

- Limitations
	1) Increased server load: Sticky sessions can increase the load on servers if there are a large number of concurrent users.
	2) Reduced scalability: Sticky sessions can make it difficult to scale an application, as the load balancer cannot distribute requests evenly across all servers.
	3) Increased complexity: Sticky sessions can increase the complexity of an application, as the application must be able to handle requests from multiple servers.


- DynamoDB
----------
	- dynamoDb datatypes supports
		- String
			- A sequence of characters.
			- The maximum length of a string is 256 KB.
		- Number
			- A numeric value.
			- The supported number types are integers, floating-point numbers, and decimals.
		- Binary
			- A sequence of bytes.
			- The maximum length of a binary is 1 MB.
		- Boolean
			- A value that can be either true or false.
			- There are no limitations on the use of booleans.
		- Null
			- A value that is not defined.
			- Nulls are not allowed in primary keys or sort keys.
		- List
			- A collection of items.
			- The maximum size of a list is 1 MB.
		- Map
			- A collection of key-value pairs.
			- The maximum size of a map is 1 MB.
		- Set
			- A collection of unique items.
			- The maximum size of a set is 1 MB.

- aws simpleDB vs dynamodb
---------------------------
	- Both are NoSQL databases

		------------------------------------------------------------------------------
		Feature			SimpleDB					DynamoDB
		------------------------------------------------------------------------------
		Data model		Simple key-value pairs		Flexible schema
		Performance		Good for low-volume apps	Good for high-volume applications
		Scalability		Scales horizontally			Scales vertically and horizontally
		Cost			Less expensive				More expensive
		Ease of use		Easy to use					More complex to use
		------------------------------------------------------------------------------

Interview Questions Bank
========================
Q1) Design a serverless API with DynamoDB or RDS as a backend.
Ans) My design steps are:
	Step 1: Use API Gateway to create the API endpoints. API Gateway will trigger the Lambda functions and handle the request/response flow.
	Step 2: Create Lambda functions to handle the API logic and interact with DynamoDB. For example,
		- GET function to retrieve an item by primary key
		- POST function to create a new item
		- PUT function to update an existing item
		- DELETE function to remove an item
	Step 3: Set up DynamoDB tables to store the data. Include primary keys and indexes as appropriate for the access patterns.
	Step 4: Give Lambda functions permissions to read/write to DynamoDB.
	Step 5: Use DynamoDB SDK in Lambda functions to perform CRUD operations:
		- GetItem/PutItem to read/write individual items
		- Query/Scan to retrieve multiple items
		- UpdateItem to update attributes
		- DeleteItem to delete an item
	Step 6: Return appropriate HTTP status codes and responses from Lambda.
	Step 7: Set up error handling in the Lambda functions to handle issues gracefully.
	Step 8: Configure CORS on API Gateway to enable cross-origin requests.


Q2) What are some of limitations with AWS Lambda and how have you solved them in the past.
Ans) AWS Lambda Limitations include:
	a) Execution duration: Lambda functions are limited to 15 minutes max. For long-running processes, I've either broken work into segments under 15 mins or switched to using Docker/ECS.
	b) Cold starts: Spinning up a new container for each invocation can mean slow cold start times. I've solved this by optimizing code/dependencies, using provisioned concurrency, or switching to keeping containers warm with ECS.
	c) No persistent filesystem: Lambda functions don't have persistent local filesystem across invocations. I've stored files in S3 or used SQLite in /tmp if small reference data needed.
	d) Deployment Package Size: The default deployment package size is 50 MB. To better resuse, I've used Lambda Layers for reusing dependencies across multiple lambda functions.
	d) Memory limits: Max memory allocated to Lambda functions within VPC is 10GB, and for functions not deployed within a VPC is 3GB; which can be insufficient for memory intensive apps. I've optimized memory usage or distributed processing across Lambda functions.

1. GUI is being called by a lot(millions of customers). You have a currency of lets say 100 requests per second and lets say your API response time should be less than a second and How do you build? what kind of API will you build ?and what's that model look like? just walk me through.

Ans : prepare on the tools AWS route 53, APIs tuning for 100 session


1a. What does the end point look like ?
2b. Do you know the parameter type that passes to the API, what kind of parameters you'll let the user pass through the GUI? What kind of method will you use for API calling?
Ans : customer ID , Get method


2. What kind of layers that you will build in your Python for taking the calls from UI? What is exception handling? How do you maintain the concurrency of your API?

Ans : {may be not 100% correct}---> research your own  correct answer
API token authentication we will do.
Parallel processing we will use to handle multiple requests.

2a. How do you handle parallel processing?
Ans : used a multi multi-processing library of Python.---> research your own  correct answer
        2a.a : what is a multi multi-processing library.
        2a.b : So how does "multiprocessing"  work ? You get a request for your end point and how would that multiprocessing library keep track of all that until you respond back to the UI ?
        2a.c: What happens if a thread (defect) gets stuck, what kind of exception you'll catch.
2a.d How about the query? You have a lot of records per customer that you need to query and then process right, so,how do you get the response quick? and what kind of querying that you use in your API?(because you can go and query and then filter the records and then you need less than a second response time,so how do you do that)?
Ans : --> research your own  correct answer

3. I have an array , an array will have both Positive and Negative numbers and if I want to get the closest 2 numbers of zero (ex: [3,7],13,19) and how do you get and what's the pseudo code that you write?

4. Do you have any experience in AWS ? What are the services hands on experience do you have ? (ex : EC2 , Lambda)
4.a : If I want to write lambda what is the start point for lambda ?
4.b : how do you invoke the lambda ? Where does the lambda start? What's that method name?
Ans : https://docs.aws.amazon.com/lambda/latest/dg/lambda-invocation.html --> research your own  correct answer

5. How long can you run Lambda ? What's the time limit for Lambda?
Ans : 15 minutes is the limit --> research your own  correct answer

6. If I want to invoke a lambda, let's say you wanted to pull the files from S3 and you are getting an issue, let's say 403 forbidden. What kind of issue could that be ? How do you debug that, what's your resolution?
7. If I am on the Linux box and want to write a python code to download some S3 files and do some processing and then upload files to S3, How do you write a script using python to do that CLI ratcheting.

ans : there is a library called bolt23, use access token/Ws --> research your own  correct answer
        7a. I still got the access forbidden/ what could have been the issue/lets say access token is good.


Azure Cloud tools
	Azure functions

	Azure cognitive search
	Azure delta watcher
	Azure VMs
	Azure database – mainly the
	powerBI connectivity pipeline

	Worked with python module - azure-python-sqk
