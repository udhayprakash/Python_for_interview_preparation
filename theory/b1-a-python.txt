Python Importance
=================
- simple syntax, and english like language
- readable by, even, non-programmers
- Supports 3 programming paradigms
	- Aspect oriented - like how is write sequentially, and
	- Functional oriented, and
	- Object oriented
- Though we may not create classes, all time, but is called object oriented programming, as all variables/functions are treated as objects, in Python.
- Open sources and has a lot of modules, ranging from astronomy, till biotechnology

python 2 vs 3
=============
- In python2, default encoding is ascii, whereas in Python 3, default is utf-8
- In python2, dictionaries are unordered, whereas from python 3.6, dictionaries will retain the assigned order.
- Python 3 uses absolute imports by default instead of relative imports.
- Python 2 supports both old style classes and new style classes when inheriting object; whereas python3 supports only new-style classes.
- In python2, range, map, filter will return a list; where these are iterators in python3.
- For range,
	In python 2,
		- range will give list of integers
		- xrange will give a xrange object, which will give values only when iterated.
	- In python3,
		- xrange is renamed as range.
		- so, it results in range data type, with lazy or on-demand loading

python 2 to 3 conversion
========================
- I used python built-in module, lib2to3 , for migrating from python2 to python3.
- Wrote code with six module to write compatible code, for both python2 and 3, during migrating phase.
- Challenges during conversion
	- In python2, we can compare string and int; We cant do the same with python3
		- I wrote wrapper to handle it
	- DEPENDENCY MANAGEMENT is another difficult thing, as some modules are either not available in python3,
		or the latest features are not available in python3 modules.

Python Modules
===============
Requests     : for consuming APIs
Re           : for regular expressions
Scikit-learn : for machine learning - I used for creating linear regression model
BeautifulSoap: for parsing html content. Used when I worked on web crawling/Scraping
NumPy        : Advanced linear algebra and NumPy array operations
Matplotlib   : Visualisation and data plotting in 2 or 3 dimensions.
IPython      : Increasing console interactivity.
Jupyter notebook: for interactively exploring the datasets and other functions
Pandas       : Data manipulation and analysis, mainly through dataframes and tables.
boto3        : For connecting and managing AWS Cloud resources
paramiko	 : for connecting to linux machines
fabric		 : for connecting to windows machines

file system modules like
	csv files with csv and pandas modules
	json files with json and pandas modules
	yaml files with pyyaml module
	Excel files with openpyxl, Xlsxwriter, xlrd and pandas modules
	pdf files  with reportlab, pypdf2 modules
	image files with Pillow and tesserect modules

Frameworks
==========
- For web development and REST APIs - Django, flask, fastapi
- For data analytics & t

Python Questions
================
- Basic Data Types - Int, Float, Str, None, bool
- Built-in Data Structures - List, Tuple, set, Dictionary

- List
	- Mutable object - it is editable. we can add, remove any element in it.
	- indexed and retains the order of initialization
	- will retain duplicates and can store any data type
	- Attributes - append, extend, insert
- Tuple
	- same as a list, but once created, we can't change any element in it .
	- it is immutable object

- Set
	- mutable object - we can edit it
	- unordered and wont store duplicates
	- will store only immutable objects(int, float, string, frozenset, tuple)
	- frozenset is same like set, but is immutable

- Dictionaries
	- They store in key: value relation
	- keys should be immutable and unique, and hashable
	- Values can be mutable
	- They are called as HashMap in other languages
	- ex: {1:2, 3:4, 5:6}

- conclusions
	- List is suitable when the order of storage is needed and/or if there were more writes.
	- Dict suitable when there were more reads
	- Set is suitable when unique values are needed and/or set operations like union, intersection, etc are needed.
	- Tuple is suitable when there need to be immutability

- mutable vs immutable
	Any object that supports changes on itself, is mutable object; else an immutable object.
	Examples of mutable objects include list, set, dictionary
	Examples of immutable objects include tuple, frozenset, basic data types, class with __slot__ defined

- hashable
	- An object is hashable if it has a hash value which never changes during its lifetime

- List vs Tuple
	- List is mutable object, means we can edit the object like append, delete, ..
	- Tuple are faster than list
		- Tuples are stored in a single block of memory. Tuples are immutable so,
		It doesn't require extra space to store new objects.
		- Lists are allocated in two blocks:
			1. fixed one with all the Python object information, and
			2. variable sized block for the data.
	- Tuples is that they use less memory where lists use more memory
	- USAGE:
		List is used when we need to make changes in code, say adding values while loop .
		Tuple is used when we wont change something in the code

- append vs extend
	- both adding elements in last of list
	- with append, we can add either single element or an iterable
	- it will add at the end as it is
	- whereas extend works with iterable objects only, but adds in same dimension

- Slicing
	- slicing works on any iterable and indexable object, like list, tuple, str, etc.
	- syntax is in square braces,
			[startIndex, endIndex, step]
	- It will result in all elements from startIndex, till the endIndex, on counts of the given step
	- Default
		step is +1
		startIndex is 0
		endIndex is end of length

- List vs Dict
	- list is suitable when the order of elements is preferred.
	- dict is suitable when there were more reads. dict has O(1) for search an element, as
	  it stored each element uniquely with its hash.
	- Insertion time is less in list

- list.sort vs sorted
	- List.sort – will do inplace sorting. So, changes will be made on the list itself.
	- sorted() function will created a new sorted list. So, existing list will not change.

- range vs xrange
	- xrange of python2, is renamed as range in python 3
	- range in python2 results a list, whereas in python 3, range results in range object.
	- range takes three arguments - start number, stopNumber and step ;
	  then, will return the sequence for those values

- Pickling
	- Pickling means Python object to flat file conversion
	- unpickling means flat file to python object conversion
	- This process is also called serialization/deserialization
	- Other serialization formats are json, yaml, ...
	- we use it when we need to pass python object to other language processes, and vice versa.

	json.dumps for converting python string to JSON string
	json.dump  for converting python string to JSON file

	json.loads for loading JSON string to python string
	json.load  for loading JSON string to python file

- lambda function
	- one liner function, mainly useful for evaluating a single expression that is supposed to be evaluated only once.
	- SYNTAX is lambda arguments: expression
	- We need not define a complete function. We can use it instantly for the purpose.
	- It can be passed as a parameter to a higher-order function, like filter(), map(), and reduce().
	- example
		- if we have to increment all values in a list by one,
				map(lambda x:x + 1, range(10))
	- And, coming to the con side,
		- It can’t perform multiple expressions.
		- It can easily become cumbersome, for example when it includes an if-elif-…-else cycle.
		- It can’t contain any variable assignments (e.g., lambda x: x=0 will throw a SyntaxError).
		- We can’t provide a docstring to a lambda function.

- enumerate
	- Its a build-in function, mainly used when iterating over any iterable,
	  to get the loop index.
	 - ex: say we are looping with for loop, over a string, and we need loop index too,
			for index, char in mystr:
				print(index, char)

- List comprehension
	- rewriting the for loop in a single line.
	- As it creates a code object, it will be slightly faster.
	- But, comprehension will reduce the readability of code.
	- Based on the braces we store with, comprehensions include
		- list comprehension
		- set comprehension
		- dictionary comprehension
		- Tuple comprehension is called generator expression

- *args vs **kwargs(args vs kwargs)
	- They are used for creating variadic functions.
	- *args will enable a function to take zero or any number of arguments.
	  It will store all the values in a tuple

	- **kwargs will enable the function to take any number of keyword arguments.
	  It will store all values in a dictionary.
	  Ex: func(a=1, b=2, c=3)  will create a dict {'a': 1, 'b': 2, 'c': 3}

constructor
	- its the method, which will be called, automatically, when we
	  instantiate the class instance
	  __init__() is the method for the same

destructor
	- its the method, which will be called, automatically, when we
	  delete an instance, or at the end of scope of the instance.
	  __del__() is the method for the same


overloading vs overwriting
	- If we define two functions/methods with same name, but different no. of arguments,
	  then, if we can call both functions/methods individually, it is called Overloading.

	- Python does not support overloading, but only overwriting is supported
	- Overwriting means among the defined two functions, as both have same name,
	  the latest defined function/method will overwrite others.

subprocess popen vs run
	- with subprocess.run and os.system, we can run any command, and get result as
		0 if command executed successfully
		1 if command FAILED
	- with subprocess.popen, we can capture both output and error

- range() vs xrange()
	- In python 2,
		- range will give list of integers
		- xrange will give a xrange object, which will give values only when iterated.
		- Means, xrange is lazy loading object.
	- In python3, xrange is renamed as range.

- copy vs deepcopy / Shallow vs Deep copy
	- This problem occurs in mutable objects i.e., list, dictionary
	- when working with mutables, when we assign a
			list1 = list2
		It will not create new object, but both will refer to the same object.
		So, if you change in one list, it will reflect in others too.
		To avoid this leakage, I used copy module

	- Shallow copy is used for single dimension mutable objects
	- Deep copy is used for multiple dimension mutable objects

	- Reason:
		when we do assignments of mutable objects, say like,
			mylist2 = mylist1
		Then, if we make changes of one of them, the others will also be affected.
		So, we use copy module to safely copy.
		Shallow copy or soft copy will also have leakages for multi-dimensional mutable objects.
		so, conclusion is that
			shallow/soft copy need to used for single dimension mutable objects, as it is faster.
			Deep copy need for multi-dimensional mutable objects.

- try-exception blocks
	1) Try   --- executed every time
	2) Except--- executed when there is exception in try block,
				 check if it can handle break code if it cant

	3) Else  --- executed when no exception in try block
	4) Finally-- executed every time  — irrespective of failure  –
				 it is like for file closing, etc ….

- Debugging in python
	- Either using pdb or ipdb modules, or using breakpoint() function

- error vs exception
	- errors are fatal problems that cannot be handled by the program, while
	- exceptions are non-fatal problems that can be handled by the program

	- error
		- An error is a problem that occurs at runtime and causes the program to terminate abnormally.
		- Errors are typically caused by problems with
			- system resources, such as memory allocation errors or file I/O errors, or
			- by programming errors, such as dividing by zero or accessing an array out of bounds.
		- Errors are usually fatal and cannot be handled by the program.

	- exception
		- is a problem that occurs at runtime, but can be handled by the program.
		- Exceptions are typically caused by
			- errors in program logic, such as attempting to open a file that does not exist, or
			- by unexpected input or conditions.
		- When an exception occurs, the program stops executing the current function and looks for an exception handler that can handle the exception.
		- If no handler is found, the program terminates with an error.

OOP
====
- OOP analogy
	1) Objects are like real-world things.
	   For example, a car is an object. It has properties (e.g., color, make, model), and it can perform actions (e.g., drive, turn, stop).
	2) Classes are like blueprints for objects.
		They define the properties and actions that objects of that class can have. For example, the class "Car" would define the properties "color", "make", and "model", and the actions "drive", "turn", and "stop".
	3) Inheritance is like a car being based on another car.
		For example, a sports car is a type of car. It inherits all of the properties and actions of a car, but it also has its own unique properties and actions (e.g., a sports car might have a spoiler or a sunroof).

- oop (object Oriented Programming)
	- I use Object-Oriented Programming and Design predominantly.
	- It gives us the scalability in code to adapt any requirement changes.

	- There are 4 principles:
	1) ENCAPSULATION
		- Encapsulation is used to restrict access to methods and variables.
		- In encapsulation, code and data are wrapped together within a single unit from being modified by accident.
		- with public, protected and private variables/methods
			public variable -- it a variable is starting with no underscore
			protected variable -- if it is starting with SINGLE underscore
			private variable -- if it is starting  with TWO underscores

		We can access all variables  of all classes irrespective of public or private or protected.
		This name mangling is to avoid accidental override

	2) ABSTRACTION:
		- It is defined as a process of hiding implementation details and showing only functionality to the user.
		- It is defined as a process of handling complexity by hiding unnecessary information from the user.

	3) INHERITANCE:
		- python supports single, multiple and multi-level inheritance, using MRO (Method Resolution Order)
				single ---> one parent class & one child class
				multiple --> two parent classes & one child class
				multi-level -->  parent1 => child ==> subChild
		- Method Resolution Order is the order in which base classes are searched for a member during lookup.
		  hybrid & hierarchical

	4) POLYMORPHISM: It is an ability of object to behave in multiple form.
		- For better code resusability, common code we write in one class, and inherint in other classes.
		- Python supp

		Ex: + operator, It will work differently in different contexts
				addition operation between integers
				List concatenation operator between lists
				Tuple concatenation operator between tuples
		operator overloading allows to redefine the behavior of native operators (+,-, *, /, +=, -=, **, etc.)
		It means we can create code with greater readability, since we use native operators to define new representations, comparisons, and operations between objects that we have created.
		Ex: Using dunder methods  __add__ and __sub__, we can use + and - opertors respectively on the class instances

- self
	- It is the first argument in instance methods.
	- Its the placeholder for the instance being passed to the class
	- It is not a keyword, but PEP 8 recommends as everyone is using, for uniformity

- class variables vs instance variables
	- Class variable is used to work with class methods
	- Instance variables are just associated with instance methods

	- Say, we have created  5 instances  from a car class
	  We need to give unique id to all the car instances,
	  Then, We can track using the class variables
	- Instance variables of each instance, will be isolated
	  from that of other instances.

- class method vs static method
	- Methods are of three types
		1) Instance Methods have self as first argument, and use instance variables/methods in it.
		2) Class Methods have cls as first argument, and use class variables/methods in it.
		3) Static Methods use neither class nor instance variables/methods. It is like a utility method.

	- cls is not keyword yet, but recently self is declared as a keyword in python 3.11.
	  - yet, it is best practice to use only them, by PEP8

	- staticmethods are mainly used for utiity functions

- function vs method
	- methods are defined in a namespace of a class; so associated with that class.
	- functions are defined independently

- Abstract Classes
	- These are called as blueprint for other classes
	- These classes are used for inheritance only, and not for creating instances.

- Virtual environment
	- It is like a container, for isolating the project dependencies.
	- When in same server, we need to execute different versions, Or same python version, with different module versions, we go with a virtual environment.
	- virtual environments can be created using modules like
		- virtualenv
		- pipenv, or
		- poerty

	- Using virtualenv module,
		command to create virtual enviromment is
			virtualenv venv

		To activate the virtual enviromment,
			venv/scripts/active - in windows
			venv/bin/active - in linux/mac

- .py vs .pyc files
	❏ .py files contain the source code of a program. Whereas, .pyc file contains the bytecode of your
	program.
	❏ Python compiles the .py files and saves it as .pyc files, so it can reference them in subsequent
	invocations.
	❏ The .pyc contain the compiled bytecode of Python source files. This code is then executed by
	Python's virtual machine.

- Generators
	- are the functions with yield, instead of return
	- They are useful for large computations, like reading a large file, or creating 1000 prime numbers, or so.
	- Generators follow the "STATE SUSPENSION"
		- we can get one value at a time using next() builtin function,
			or iterate , or convert to list, tuple or set, to get all values
	- return vs yield
		- return is the last statement to execute in any function
		- yield is a keyword in python
			- If yield is placed within function definition, it becomes generator

		- PEP8 - don't use both return & yield in same function

	- what happens if return is placed in generator?
	- Ans) It wont throw error. But when all values were iterated from it, for the last iteration, the return value will come as stop iteration exception message.

- Iterator
	- Any python iterable object can be converted to an intertor using iter() protocol
	- iterator can be created from any iterable object

- Decorators
	- Generally, when some code is repeating, we will create a function, reuse the code.
	- If there were more than one function, with some part of the code which is common among all, in that case, we extract common code, create a closure for it, and use it as a decorator.
	- built-in decorators are classmethod, staticmethod and property
	- Also, I created custom decorators for calculating the time taken by functions, exception-handling, or like logging the events, etc
	- Advantage - better code reusability
	- No performance change, as same code lines will execute

- decorator wraps
	- Decorators are the function which takes another functions as arguments.
	- The docstrings of the orginal function which is being passed into decorator function will be lost.
	- functools.wraps decorator on the inner function, of decorator, will help retain those docstrings.

- Map
	- Higher-order function, which is designed to work on other functions
	- map will superimpose a function on one or more iterables
	- say, we want to get double of all values in a list
		map(lambda x:x * 2, range(9))

	- In python2, map will result a list, where as in python3, it will return an iterator object.
	- Other higher order functions are Filter, Zip and Reduce

- Filter
	- This higher-order function will superimpose a function, on one or more iterables
	  and return the values, for whom the function returns boolean true.

- Zip
	- It will create pairs, on two or more iterables.
	- It will create pairs to the minimum size, among the iterables;
	  and discard asymmetric size values.

- Reduce
	- It will iterate on a single iterable with the given function
	- It will take a function and work with every two consecutive elements in that iterable
	- Finally, returns a single value as result.
	-  Ex: for calculating factorial of a number, etc.
	- Its a build-in function in python2, whereas in python 3, it is part of functools module

- pass, continue and break
	- pass is like a TODO. It does nothing, except to hold the syntax
	- say we need to have a class, or a condition, but want to write logic in future,
	  we use "pass" to retain the syntax.

	- continue will skip the current loop, whereas
	  break will stop the entire loop

- fruitful function
	- function that returns a value or a result.
	- It performs some computation and provides an output that can be used by the calling code.

- Pure functions
	- functions, which, will not modify any of the objects passed to it as parameters, and
	-  It has no side effects, such as updating global variables, displaying a value, or getting user input

				def add_time(t1, t2):
					h = t1.hours + t2.hours
					m = t1.minutes + t2.minutes
					s = t1.seconds + t2.seconds

					if s >=60:
						s -= 60
						m += 1
					if m >= 60:
						m -= 60
						h += 1
					sum_t  = MyTime(h, m, s)
					return sum_t
- Modifier functions
	- functions, which, will modify one or more of the objects it gets as parameters.

				def increment(t, secs):
					t.seconds += secs

					if t.seconds >= 60:
						t.seconds -= 60
						t.minutes += 1
					if t.minutes >= 60:
						t.minutes -= 60
						t.hours += 1

- race condition
	- A race condition occurs when two or more threads can access shared data and they try to change it at the same time.
	- Because the thread scheduling algorithm can swap between threads at any time, you don't know the order in which the threads will attempt to access the shared data.

- GIL Problem
	- GIL means Global Interpreter Lock
	- Simply, it means "One thread runs Python, while others sleep or await I/O."

	- In Cpython, as it goes with reference based assignment, if two threads are trying to update, say, the same list, it can lead to data races, and deadlock.
	- To avoid deadlocks, it will lock a single thread, safely execute it, and release it.

	It is a mutex (or a lock) that allows only one thread to hold the control of the Python interpreter. This means that only one thread can be in a state of execution at any point in time

	- Due to this, we can't achieve concurrency with python, unlike other languages.

	- Next solution is multiprocessing. But, it is resource exhaustive. And, the maximum limit on the processes which can be created, is limited to the system hardware limits.
	- So, from python 3.6, with the advent of async and await keywords, I am using the asyncio based non-blocking execution.
	- Here, within the single thread event loop, we will invoke all child threads; which is a solution to this GIL problem

	Conclusion is that
		- Multithreading works better for IO-bound tasks
			- reading/writing from files/db, etc
		- Multiprocessing works better for CPU-bound tasks
			- computationally-intensive tasks such as compression or hashing.

- Multithreading in Python
	- I Used multithreading module for it.

	- first, create all the threads
	- then, start all these threads
	- Finally, join all these threads, such that it will ensure to go to next step, after all were done.

- Asyncio
	- Asyncio allows us to run IO-bound tasks asynchronously to increase the performance of our program.
	- Common IO-bound tasks include, like,
		- calls to a database,
		- reading and writing files to disk, and
		- sending and receiving HTTP requests.

- Threading vs Asyncio
	Threading
		- In threading, OS actually knows about each thread and can interrupt it at
			any time to start running a different thread.
		- It is Preemptive Multitasking
		- code in the thread doesn't need to do anything to make the switch.
		- difficult as OS can pause/resume thread "AT ANY TIME"
	Asyncio
		- Cooperative Multitasking
		- Tasks must cooperate by announcing when they are ready to be switched out.
		- You always know where your task will be swapped out.
		- Suitable for event-driven architectures

process vs thread
	- processes do not share memory and run on a single core.
	  They are better for compute-intensive tasks that do not have to communicate with each other.
	- threads share memory.
	  In Python, due to the Global Interpreter Lock (GIL), two threads cannot operate
	  at the same time in the same program.
	  As a result, only some operations can be run in parallel using threads.

	- A process is just an instance of an executing program.
		A thread, on the other hand, is like a mini process.
	Each process typically has 1 thread of control, but in several situations, it is beneficial to have multiple threads of control in the same process.


- thread vs process
	-------------------------------------------------------------------------------------------
		PROCESS								    |			THREAD
	-------------------------------------------------------------------------------------------
	- Processes are heavyweight operations.	    | Threads are lighter weight operations.
	- Each process has its own memory space.    | Threads use the memory of the process they belong to.
	- Inter-process communication is slow as    | Inter-thread communication can be faster than inter-process communication
	  processes have different memory addresses.| because threads of the same process share memory with the process they belong to.
	- Context switching between processes is    | Context switching between threads of the same process is less expensive
	  more expensive.
	- Processes don’t share memory with other   | Threads share memory with other threads of the same process.
	  processes.

- thread life cycle
	- Every thread will have FIVE stages.
		1) New - when thread is created, and yet to start
		2) Active  - when invoked with start() method
					- within it, there are two substates
						a) runnable - Means it is about to run, when thread scheduler gives time.
						b) running - Means when thread gets CPU, it will run, and then moves to runnable state again.
		3) Blocked / Waiting - when ever thread is inactive for sometime, temporarily
		4) Timed Waiting - means sometimes, waiting for leads to starvation; say to prioritize another thread.
		5) Terminated - Either when thread finighes its job, or abnormal termination like unhandled exception, etc.


concurrency vs parallelism
 - “Concurrency is about dealing with lots of things at once.
    Parallelism is about doing lots of things at once”

- cooperative multitasking
	- Whenever a thread begins sleeping or awaiting network I/O, there is a chance for
	  another thread to take the GIL and execute Python code.

- preemptive multitasking
	- CPython has it. Something similar to time slicing
	- If a thread runs uninterrupted for 1000 bytecode instructions in Python 2, or runs
	  15 milliseconds(set via sys.setswitchinterval()) in Python 3, then it gives up
	  the GIL and another thread may run.

- Thread Pooling
	- Assiging a bunch of threads, and reusing them, instead of creating a new thread.
	- The pool is responsible for a fixed number of threads.
		- It controls when the threads are created, such as just-in-time when they are needed.
		- It also controls what threads should do when they are not being used, such as making them wait without consuming computational resources.

- Thread Safety
	- If a thead loses GIL at any moment, we should ensure that code is thread safe.
	- Many Python operations like sort(), ... are atomic.
	- GIL wont switch in middle of automatic operations.
	- EX: A thread cannot be interrupted in the middle of sorting, and other threads never
	      see a partly sorted list, nor see stale data from before the list was sorted.

	- Locks are needed to protect shared mutable state

Atomic Operations
	- All  single bytecode instruction. We can checking using dis module; dis.dis(object)
	- Assuming L, L1, L2 are lists, D, D1, D2 are dicts, x, y are objects, i, j are ints,

	L.append(x)
	L1.extend(L2)
	x = L[i]
	x = L.pop()
	L1[i:j] = L2
	L.sort()
	x = y
	x.field = y
	D[x] = y
	D1.update(D2)
	D.keys()

PYTHONPATH
	- It is an environment variable that you set before running the Python interpreter.
	- PYTHONPATH, if it exists, should contain directories that should be searched for modules when using import.
	- If PYTHONPATH is set, Python will include the directories in sys.

	- It is a special environment variable that tells Python interpreter about where to find various libraries and applications.

Non-Automic Operations
	i = i+1
	L.append(L[-1])
	L[i] = L[j]
	D[x] = D[x] + 1

- context managers
	- will allow you to allocate and release resources precisely when you want to.
	- Examples include when working with
		- file operations, database or socket connections, etc.
	- They implement the "finally" block once we come out of the context manager indentation.

	- we can do with the 'with' keyword, like

		with open('file_name.txt', 'r') as fh:
			data = fh.read()

	- we need not close the file handler. Once it comes out of the context indentation,
	  it will automatically close
I	- we can create custom context managers, using the dunder methods,
      __enter__() and __exit__().
	- I remember using it to track the file operations.

		class File:
		  def __init__(self, filename, method):
			self.file = open(filename, method)

		  def __enter__(self):
			print("Enter")
			return self.file

		  def __exit__(self, type, value, traceback):
			print(f"{type}, {value}, {traceback}")
			print("Exit")
			self.file.close()

		with File("file.txt", "w") as f:
		  print("Middle")
		  f.write("hello")
		  raise Exception()
		  raise FileExistsEoor()

- call by value vs call by reference
	- python supports both call by value and call by reference.

	- call by value means changes within functions, on variables, should not reflect outside.
	- call by reference means changes with functions, on variables, should REFLECT outside.

	- For immutables,
		- By default, it is call by value.
		- To make as call by reference, we need to define as global variables, to reflect local changes, outside.
	- For mutables,
		- By default, it is call by reference.
		- To avoid local changes, from reflecting outside, we need to do changes on copied objects.


- descriptors
	Descriptors are used to transform access object properties into call descriptor methods.
	The __get__() method is called when the attribute is accessed for reading.
	The __set__() method is called when the attribute is assigned a value.
	The __delete__() method is called when the attribute is deleted.

	Descriptors are typically used to define properties in a class, but they can also be used to define methods.

	USECASE:
	Consider an email attribute. Verification of the correct email format is necessary before assigning a value to that attribute. This descriptor allows email to be processed through a regular expression and its format validated before assigning it to an attribute.


__getattribute__ vs__getattr__
	__getattribute__ will intercept EVERY attribute lookup, doesn’t matter if the attribute exists or not.


- name mangling
	- python supports Public, private and protected variables/methods/functions
	- We can access even private variables, methods too  in python,
		But the convention is made to avoid accidental override

	- Variable starting with no _underscores are called public variables
	- Variables starting with single underscore are called protected variables
	- variables starting with two underscores are called private variables
		- Also, variables starting with two underscores and ending with two underscores
		  are magic methods (or dunder/double underscore methods, or special methods).
		- Ex: __init__ is the constructor
			  __del__ is the destructor
			  __enter__ and __exit__ are used for creating custom context managers
			  __iter__ and __next__ are used for creating Iterator objects
			  __str__ and __repr__ are used for string representation of the instances

		-----------------------------------------------------------------------------
		Dunder method	Usage
		-----------------------------------------------------------------------------
		__init__			Initialise object
		__new__				Create object
		__del__				Destroy object
		__repr__			Compute “official” string representation / repr(obj)
		__str__				Pretty print object / str(obj) / print(obj)
		__bytes__			bytes(obj)
		__format__			Custom string formatting
		__lt__				obj < ...
		__le__				obj <= ...
		__eq__				obj == ...
		__ne__				obj != ...
		__gt__				obj > ...
		__ge__				obj >= ...
		__hash__			hash(obj) / object as dictionary key
		__bool__			bool(obj) / define Truthy/Falsy value of object
		__getattr__			Fallback for attribute access
		__getattribute__	Implement attribute access: obj.name
		__setattr__			Set attribute values: obj.name = value
		__delattr__			Delete attribute: del obj.name
		__dir__				dir(obj)
		__get__				Attribute access in descriptor
		__set__				Set attribute in descriptor
		__delete__			Attribute deletion in descriptor
		__init_subclass__	Initialise subclass
		__set_name__		Owner class assignment callback
		__instancecheck__	isinstance(obj, ...)
		__subclasscheck__	issubclass(obj, ...)
		__class_getitem__	Emulate generic types
		__call__			Emulate callables / obj(*args, **kwargs)
		__len__				len(obj)
		__length_hint__		Estimate length for optimisation purposes
		__getitem__			Access obj[key]
		__setitem__			obj[key] = ... or `obj[]
		__delitem__			del obj[key]
		__missing__			Handle missing keys in dict subclasses
		__iter__			iter(obj) / for ... in obj (iterating over)
		__reversed__		reverse(obj)
		__contains__		... in obj (membership test)
		__add__				obj + ...
		__radd__			... + obj
		__iadd__			obj += ...
		__sub__				obj - ...
		__mul__				obj * ...
		__matmul__       	obj @ ...
		__truediv__       	obj / ...
		__floordiv__       	obj // ...
		__mod__       		obj % ...
		__divmod__			divmod(obj, ...)
		__pow__       		obj ** ...
		__lshift__       	obj << ...
		__rshift__       	obj >> ...
		__and__       		obj & ...
		__xor__       		obj ^ ...
		__or__       		obj | ...
		__neg__				-obj (unary)
		__pos__				+obj (unary)
		__abs__				abs(obj)
		__invert__			~obj (unary)
		__complex__			complex(obj)
		__int__				int(obj)
		__float__			float(obj)
		__index__			Losslessly convert to integer
		__round__			round(obj)
		__trunc__			math.trunc(obj)
		__floor__			math.floor(obj)
		__ceil__			math.ceil(obj)
		__enter__			with obj (enter context manager)
		__exit__			with obj (exit context manager)
		__await__			Implement awaitable objects
		__aiter__			aiter(obj)
		__anext__			anext(obj)
		__aenter__			async with obj (enter async context manager)
		__aexit__			async with obj (exit async context manager)

- logging

  log levels
	Debug
	Info
	Warning
	Error
	Critical

  log levels can be set either in basic config, or in the log object.


- Monkey patching is replacing a function/method/class by another at runtime,
   for testing purpses, fixing a bug or otherwise changing behaviour.

	Usage:
		monkeytype run <file>.py  dumps function call traces into SQLite databse
		monkeytype stub <module>.py outputs preview of annotated code
		monkeytype apply <module>.py applies the annotated code to the file

- namespaces
	LEGB - Local, Enclosed, Global, Builtin scopes

	And a class is not a scope entity in it’s own right - which means that attributes of a class or instance must be qualified by either ‘self’ or a specific class name.


- py vs pyc
	- py extension files are ordinary python files
	- pyc extension files are python bytecode files,

- PEP 8
	- PEP is abbreviation for Python Enhancement Proposal
	- It deals with the coding style guide
	- It will list the best practices, in coding in python
	- These recommendations include
		- to use 4 spaces, and not tabs , for indentation
		- Use triple quotes for docstrings
		- Wrap lines so that they don’t exceed 79 characters, in each line
		- class names should in camel casing, and all others should be in underscore casing
		- Use Python’s default UTF-8 or ASCII encodings and not any fancy encodings
		- Blank Lines
			- Two blank lines should be both before and after class/method/function definitions.
			- You should use blank lines conservatively within your code to separate groups of functions.
		- One space around the operators
		- Wildcard imports (*) should not be preferred
		- Avoid trailing white-spaces

- Memory Management in python
	- Python has automatic memory management, based on garbage collector.
	- cpython uses reference counting to detect inaccessible objects, and another mechanism to collect reference cycles,
	  periodically executing a cycle detection algorithm which looks for inaccessible cycles and deletes the objects involved.
	- We have a garbage collector, which will check for any unreferenced objects
	  on a periodically, for each CPU clock cycle, and it will delete them.

	- Reference count is the number of references to an object.
	  When the reference count of an object drops to zero, it is deallocated.

	- we can check the reference count of any value using the sys module, sys.getreferencecount
	- The garbage collector can be controlled using the gc module.

- stack & heap
	- each thread will have its own stack, but all the threads in a process will share
	  the heap.

Heap vs stack
--------------
	- Whenever an object is created, it’s always stored in the Heap space, and stack memory contains the reference to it.
	- Stack memory only contains local primitive variables and reference variables to objects in heap space.
	- Objects stored in the heap are globally accessible whereas stack memory can’t be accessed by other threads.
	- Memory management in stack is done in LIFO manner whereas it’s more complex in Heap memory because it’s used globally.
	- Stack memory is short-lived whereas heap memory lives from the start till the	end of application execution.
	- Heap memory is used by all the parts of the application, stack memory is used	only by one thread of execution.
	- When stack memory is full, Java runtime throws java.lang.StackOverFlowError
	  When heap memory is full, it throws java.lang.OutOfMemoryError: Java Heap Space error.
	- Stack memory is faster than heap memory.

global variables in python
--------------------------
	- I use Enum for storing the global variables.
	- I will segrgate them based on the usage.
	- will store them in __init__.py file, and load at the package level.


- static class vs singleton
---------------------------

	------------------------------------------------------------------------------------------------------
	Singleton Pattern														Static Class
	------------------------------------------------------------------------------------------------------
	1) Singleton is a design pattern.										Static classes are basically a way of grouping classes together in Java.
	2) Memory is allocated once the object is created.						Memory is allocated immediately after any of the class members is accessed.
	3) Singleton implementation can either have static members or instance members.	Static classes can contain static members only.
	4) It can implement any other interface or base class is required.		It cannot implement the interface or any other base class.
	5) Singleton classes can be used as a method parameter.	Static class cannot be used as a method parameter.
	6) Singleton pattern uses Heap memory.	Static classes use stack memory.
	7) It works within the scope of Garbage Collector as it uses Heap memory.	Out of scope for Garbage Collector as it uses stack memory.
	8) It supports Dependency Injection (DI) implementation as Singleton follows OOPS concepts.	It cannot implement Dependency Injection (DI) as DI is Interface-driven.
	9) Singleton is an architectural pattern and not a language feature.	Static is a language feature and not an Architectural pattern.
	10) Disposal of objects is possible.									It cannot dispose of the static class as there is no instance created.


Python secure coding best practices
------------------------------------
	1) Always sanitize external data
		- to avoid injctions, cross-side scripting, or Denial of Service(DoS) attacks
	2) Scan your code
		- linting with flake8, etc
		- scanning with bandit, etc
	3) Be careful when downloading packages
		- To avoid typesquatting (mean malicious packages with similar name of legitimate packages)
	4) Review your dependency licenses
	5) Do not use the system standard version of Python
	6) Use Python’s capability for virtual environments
	7) Set DEBUG = False in production
	8) Be careful with string formatting
	9) (De)serialize very cautiously
		- using SafeLoader option, instead of Loader, when loading json, yaml, pickle, etc.
		- This prevents loading of custom classes but supports standard types like hashes and arrays.
	10)Use Python type annotations
		- as it will prevent most of runtime errors.

Python code performance optimization
------------------------------------
	1) Use built-in functions and libraries: Python has a lot of built-in functions and libraries that are optimized for performance. Using them can often be much faster than writing your own code.
	2) Use list comprehensions and generators: List comprehensions and generators are more efficient than using loops, especially when dealing with large data sets.
	3) Use profiling: Profiling can help identify performance bottlenecks in your code. The Python standard library includes the cProfile module, which can be used to profile code.
	4) Parallelize your code: Python has a number of libraries for parallel processing, such as multiprocessing and threading. Parallelizing your code can help improve performance on multi-core systems.
	5) Use caching: If a function is called repeatedly with the same arguments, consider caching the result to avoid repeating the same calculation.
	6) Avoid unnecessary function calls: Function calls in Python are relatively expensive. Avoid calling functions unnecessarily, especially in loops.
	7) Use appropriate data structures: Choosing the right data structure can significantly improve performance. For example, using sets instead of lists can improve performance when dealing with membership testing.
	8) Use NumPy for numerical calculations: NumPy is a library for numerical calculations that is optimized for performance. Using NumPy can be much faster than using Python's built-in data types.
	9) Consider using a JIT compiler: A JIT (Just-In-Time) compiler can significantly improve the performance of Python code. Libraries such as Numba and PyPy include JIT compilers.

	conclusion - There are many techniques; but performance can sometimes come at the cost of readability and maintainability.
		ex: List comprehension is fast, compared to ordinary for loop; but at the cost of readability.

Loops - Performance
===================
In General, there are three types of loops
	1) while loop will always evaluates the condition first
	2) do/while loop will always execute the code in the do{} block first and then evaluate the condition.
	3) for loop allows to initiate a counter variable, a check condition, and a way to increment your counter all in one line.

	while is faster, as for loop need to compute the limits, before starting the loop.


Metaclasses
-----------
	- A metaclass is the class of a class.
	- A class defines how an instance of the class (i.e. an object) behaves
		while a metaclass defines how a class behaves. A class is an instance of a metaclass.

	While in Python you can use arbitrary callables for metaclasses, the better approach is
	to make it an actual class itself.
	type is the usual metaclass in Python.
	type is itself a class, and it is its own type.
	You won't be able to recreate something like type purely in Python, but Python cheats
	a little. To create your own metaclass in Python you really just want to subclass type.

	A metaclass is most commonly used as a class-factory. When you create an object
	by calling the class, Python creates a new class (when it executes the 'class' statement)
	by calling the metaclass.
	Combined with the normal __init__ and __new__ methods, metaclasses therefore allow you to
	do 'extra things' when creating a class, like registering the new class with some registry or replace the class with something else entirely.

	When the class statement is executed, Python first executes the body of the class statement as a normal block of code. The resulting namespace (a dict) holds the attributes of the class-to-be. The metaclass is determined by looking at the baseclasses of the class-to-be (metaclasses are inherited), at the __metaclass__ attribute of the class-to-be (if any) or the __metaclass__ global variable. The metaclass is then called with the name, bases and attributes of the class to instantiate it.

	However, metaclasses actually define the type of a class, not just a factory for it, so you can do much more with them. You can, for instance, define normal methods on the metaclass. These metaclass-methods are like classmethods in that they can be called on the class without an instance, but they are also not like classmethods in that they cannot be called on an instance of the class. type.__subclasses__() is an example of a method on the type metaclass. You can also define the normal 'magic' methods, like __add__, __iter__ and __getattr__, to implement or change how the class behaves.

	Here's an aggregated example of the bits and pieces:

	def make_hook(f):
		"""Decorator to turn 'foo' method into '__foo__'"""
		f.is_hook = 1
		return f

	class MyType(type):
		def __new__(mcls, name, bases, attrs):

			if name.startswith('None'):
				return None

			# Go over attributes and see if they should be renamed.
			newattrs = {}
			for attrname, attrvalue in attrs.iteritems():
				if getattr(attrvalue, 'is_hook', 0):
					newattrs['__%s__' % attrname] = attrvalue
				else:
					newattrs[attrname] = attrvalue

			return super(MyType, mcls).__new__(mcls, name, bases, newattrs)

		def __init__(self, name, bases, attrs):
			super(MyType, self).__init__(name, bases, attrs)

			# classregistry.register(self, self.interfaces)
			print "Would register class %s now." % self

		def __add__(self, other):
			class AutoClass(self, other):
				pass
			return AutoClass
			# Alternatively, to autogenerate the classname as well as the class:
			# return type(self.__name__ + other.__name__, (self, other), {})

		def unregister(self):
			# classregistry.unregister(self)
			print "Would unregister class %s now." % self

	class MyObject:
		__metaclass__ = MyType


	class NoneSample(MyObject):
		pass

	# Will print "NoneType None"
	print type(NoneSample), repr(NoneSample)

	class Example(MyObject):
		def __init__(self, value):
			self.value = value
		@make_hook
		def add(self, other):
			return self.__class__(self.value + other.value)

	# Will unregister the class
	Example.unregister()

	inst = Example(10)
	# Will fail with an AttributeError
	#inst.unregister()

	print inst + inst
	class Sibling(MyObject):
		pass

	ExampleSibling = Example + Sibling
	# ExampleSibling is now a subclass of both Example and Sibling (with no
	# content of its own) although it will believe it's called 'AutoClass'
	print(ExampleSibling
	print(ExampleSibling.__mro__)


Numpy
------
- How are numpy and pandas related
- numpy arrays

array vs list
	- List
		- list is builtin data structure in python
		- we can either have all elements of same type, or different type
	- array
		- array is available from numpy
		- IN array, we need to have all elements of same type.

	- whenever I have to store all elements of same type, as arrays are more performant than lists, I prefer to use them.

numpy vs pandas
	- numpy is mostly suitable for numerical data computations, whereas
	pandas is suitable for tabular data processing.

	- Numpy is a library that helps you do math with the data, like adding or subtracting numbers.
	Pandas is a library that helps you organize data, like putting it into tables.

- Vectorization
	- Vectorization is the technique of implementing (NumPy) array operations on a dataset.
	In the background, it applies the operations to all the elements of an array or series in one go

Pandas
------
In Pandas,
	I worked on reading csv, excel spreadsheets,
	parsing the content,
	transformations like
		added a new column based on some logic, or
		creating new column based on other columns, or
		cleansing activities like removing null rows, cleaning columns, etc

	finally, dumping the resultant data into csv, excel or, even a db table.

- Data structures, supported by pandas
	- Series - for one-dimensional data
	- dataframe - for two dimensional data

array vs dataframe
	numpy arrays are either single or multiple dimensions
	pandas has two things
		1) Series is 1 dimensional
		2) Dataframe is 2 dimensional

	Indexing of numpy arrays is faster than that of dataframe.

- loc vs iloc
	- loc gets rows (or columns) with particular labels from the index.
	- iloc gets rows (or columns) at particular positions in the index (so it only takes integers).

- dataframe index
	- When index is unique, pandas use a hashtable to map key to value O(1).
	- When index is non-unique and sorted, pandas use binary search O(logN),
	  when index is random ordered pandas need to check all the keys in the index O(N).

	- join vs merge
		- join() is used to combine two DataFrames on the index but not on columns
		- merge() is primarily used to specify the columns you wanted to join on, this also supports
		joining on indexes and combination of index and columns.

- find differences between two dataframes
	df1 = pd.DataFrame({'A':[1,2,3,3],'B':[2,3,4,4]})
	df2 = pd.DataFrame({'A':[1],'B':[2]})

	pd.concat([df1,df2]).drop_duplicates(keep=False)

- Load only the top n rows
    df = pd.read_csv("IMDB Dataset.csv", nrows=100)

- Load n rows from the middle
    df = pd.read_csv("IMDB Dataset.csv", skiprows=range(1,4), nrows=100)

- Convert List to datetime
    pd.to_datetime(pd.Series(s), format='%b_%d')
    pd.to_datetime(pd.Series(s) + '_2015', format='%b_%d_%Y')

- Setting a column as index
    df = df.set_index("column")

- To remove duplicate rows
    based on all columns
        df.drop_duplicates()
    based on specific column
        df.drop_duplicates(subset=['brand'])
    To keep last occurrences,
        df.drop_duplicates(subset=['brand', 'style'], keep='last')


- dataframe to series
- series to dataframe
- uses of series
- vectorization
multi index pandas
- Feature Engineering
	- technique to create  new features from existing data that could help to gain more
	  insight into the data.
	- pandas.apply() for text extraction
	- pandas.groupby() and transform() for Aggregation Features
	- pandas.value_counts() and apply() for Frequency Encoding

- to read specific columns
	df = pd.read_csv("sample_file.csv", usecols=col_list)


Recently, I started working on Polar, which is faster than pandas,
Also supports concurrency too.

Polar syntax is similar to pandas and pyspark, yet it is written in RUST,
So, is extremely fast and support concurrency

- pandas vs pyspark
------------------
	- Pandas run operations on a single machine whereas PySpark runs on multiple machines.
	- Pandas is best suited for small to medium-sized datasets that can fit into memory on a single machine, while PySpark is designed for large-scale data processing on distributed clusters.
	- Pandas provides a wide range of functions for data manipulation and analysis, while PySpark provides a more powerful set of APIs for distributed computing,

- RDD vs dataframe spark
-----------------------
	- RDDs are a low-level data structure with a more flexible schema,
	  while DataFrame is a higher-level data structure with a fixed schema and more efficient processing.
	- DataFrame is generally the preferred data structure in Apache Spark for most data processing tasks, as it provides better performance and a more intuitive API.
	- In terms of
		1) Data Representation:
			- RDD is a lower-level data structure that represents data as an immutable, fault-tolerant collection of elements that can be processed in parallel across a cluster.
			- DataFrame is a higher-level data structure that represents data in a tabular format with named columns, similar to a relational database table.

		2) Schema:
			- RDDs do not have a fixed schema, which means that each record in an RDD can have a different structure.
			- DataFrame has a fixed schema that defines the data types and names of columns, which enables more efficient processing of data.

		3) APIs:
			- RDD provides a low-level API that exposes transformations and actions to perform distributed computing on the data.
			- DataFrame provides a higher-level API that includes built-in operations for filtering, grouping, joining, and aggregating data, making it easier to work with structured data.

		4) Performance:
			- DataFrame is more optimized for query optimization, making it faster and more efficient than RDD for most data processing operations.
			- RDD can be more performant for certain types of operations, such as iterative algorithms or data processing with complex data structures.

- parquet vs deltalake
	- parquet cant handle the slowly changing dimensions, say scd2, which delta lake, we can do better
	- Example, we want to track the address of a person, and if name, address were the only columns,
	   we will loose the history, everytime, we update. so, we need to have start and end date against each row.

	   name, address, startDate, endDate
	   ram,   121	, 21-02-2000, 31-05-2010
	   ram,   121	, 31-05-2010, NULL

- read large parquet file
-------------------------
	- There were many ways

	1) Using the DataFrameReader API to load Parquet files as DataFrames:
			df = spark.read.parquet('large_file.parquet')

	2) Enable predicate pushdown to only read relevant partitions/columns:
		df = spark.read.parquet('data.parquet', columns=['id', 'name'])
	3) Using partitioning to break up large files into smaller partitioned files based on a column like date. 	It enables pruning partitions through pushdown.
	4) Co-partition Hive tables with corresponding Parquet data to optimize query performance.
	5) Increasing the number of cores/executors for more parallelism when reading.
	6) Using DataBricks IO cache to cache hot Parquet files in memory:
			spark.conf.set("spark.databricks.io.cache.enabled", "true")

	7) Using Parquet with ORC for even better compression. ORC indexes help skip irrelevant rows/groups.
	8) For extreme cases, down-sample Parquet files before analysis to experiment and iteratively develop models.


- repartition()
	Repartition RDDs or DataFrames into new number of partitions.
		df = df.repartition(100)

- coalesce()
	Decrease the number of partitions in RDD or DataFrame.
		df = df.coalesce(10)

- pyspark bucketby vs partitionby

	bucketBy() allows fixing the number of buckets, partitionBy() does not.
	bucketBy() attempts to distribute rows evenly, partitionBy() does not.
	bucketBy() can bucket based on a hash of column values, partitionBy() partitions based on exact column values.
	bucketBy() results in a BucketedRandomProjection, partitionBy() results in a Partitioning.

	bucketBy()
		- splits the DataFrame into a fixed number of buckets based on the values of one or more columns
		- The number of buckets is specified by the user.
		- Rows with similar values for the bucketing columns will be placed in the same bucket.
		- bucketBy() is often used to distribute data evenly for joining or sampling.

	partitionBy()
		- splits the DataFrame into partitions based on the values of one or more columns.
		- It does not require specifying the number of partitions upfront.
		- Rows with the same values for the partitioning columns will be in the same partition.
		- partitionBy() is often used to optimize physical data layout for faster query performance.

- pyspark compression parquet
	- suported compression codes are 'none', 'uncompressed', 'snappy', 'gzip', and 'lzo'
	- Snappy is generally a good default.
			df.write.parquet('data.parquet', compression='snappy')

	- Compression can be done in different ways:
		- Specify compression per column:
				df.write.parquet('data.parquet', compression={
					'large_column': 'gzip',
					'other_columns': 'snappy'
				})

		- Configure compression globally in Spark session:
				spark.conf.set("spark.sql.parquet.compression.codec", "snappy")
				or
				sqlContext.setConf("spark.sql.parquet.compression.codec.", "snappy")

		- Enable dictionary encoding for string columns:
				df.write.option("parquet.dictionary.encoding", "true").parquet("data.parquet")

		- Use a high compression codec for partitioned columns:
				df.write.parquet(
					"data.parquet",
					partitionBy('eventDate'),
					compression={ 'eventDate': 'gzip' }
				)

Hadoop
======
	- Hadoop is an open-source big data framework for storing & processing large datasets across clusters
	- Hadoop provides distributed storage via HDFS and batch processing via MapReduce.

	- Main components
		1) HDFS (Hadoop Distributed File System):
			- Distributed and scalable filesystem for storing large datasets.
			- Provides high throughput access across clusters.
		2) YARN (Yet Another Resource Negotiator):
			- Cluster resource management for scheduling and managing jobs.
			- Allows multiple data processing engines to run on Hadoop.
		3) MapReduce:
			- Programming model for large scale data processing on Hadoop.
		4) Pig:
			- High level dataflow language and execution framework for parallel computation.
			- Provides SQL-like language for MapReduce tasks.
		5) Hive:
			- Data warehouse software for reading, writing and managing large datasets in distributed storage using SQL. Built on top of HDFS.
		6) Oozie:
			- Workflow scheduler system to manage Hadoop jobs.
			- Defines workflows and job dependencies for coordination.
		7) ZooKeeper:
			- Centralized service for distributed coordination between applications and services.
			- Enables synchronization and configuration maintenance.
		8) HBase:
			- Distributed, scalable NoSQL database that runs on HDFS.
			- Provides real-time read/write access to large datasets.
		9) Sqoop:
			- Tool to transfer bulk data between Hadoop and external structured datastores like RDBMS.
			- Useful for importing/exporting between Hadoop and external databases.
		10) Flume:
			- Distributed, reliable service for collecting, aggregating and moving large amounts of log data into Hadoop. Useful for aggregating web/app log data.
		11) Mahout:
			- Scalable machine learning and data mining library for Hadoop. Implementations of classification, clustering and collaborative filtering algorithms.

HDFS (Hadoop Distributed File System)
=======================================
	- HDFS architecture - Core components include
		1) NameNode
			- Its the master node that manages the file system metadata(file names, file sizes, and file locations,...) and access.
			- It maintains the directory tree, file metadata, and mappings of data blocks to DataNodes.
			- And, Secondary NameNode is a backup for the NameNode. It periodically synchronizes its metadata with the NameNode.

		2) DataNodes
			- are the worker nodes in the cluster that store the actual data blocks/files.
			- The blocks are replicated across multiple DataNodes for fault tolerance.
			- These are slave nodes in hadoop Cluster
		3) Client
			- The client contacts NameNode for file metadata and interacts with DataNodes directly to read/write file data.
		4) Blocks
			- Files are split into blocks of fixed size (e.g. 128 MB) and stored across DataNodes.
			- Each block is replicated for fault tolerance.
		5) Namenode and Datanodes are the Master and Slave daemons.
		6) JobTracker
			- is the master node for MapReduce jobs.
			- It schedules jobs, tracks job progress, and manages resources.
		7) TaskTracker
			- are the slave nodes for MapReduce jobs.
			- They execute MapReduce tasks and report their progress to the JobTracker.

	- HDFS architecture - Other components include
		1) Hadoop Distributed File System (HDFS):
			- HDFS is the distributed file system that stores data in Hadoop clusters.
		2) Apache ZooKeeper:
			- ZooKeeper is a distributed coordination service that is used by Hadoop to manage cluster metadata.
		3) Apache Hadoop MapReduce:
			- MapReduce is a programming model and an associated implementation for processing and generating large data sets.
		4) Apache Hadoop Hive:
			- Hive is a data warehouse infrastructure built on top of Hadoop.
			- It provides a SQL-like language for querying data stored in Hadoop.
		5) Apache Hadoop HBase:
			- HBase is a distributed NoSQL database that is built on top of Hadoop.
			- It provides a column-oriented storage model that is optimized for large amounts of semi-structured data

	- HDFS architecture follows a master-slave model:
		- The NameNode (master) manages the file system namespace and metadata.
		- The DataNodes (slaves) manage data storage on the hosts they run on.
		- Clients read/write files by contacting the NameNode for file locations and then streaming the data from/to the DataNodes.
		- The cluster can scale horizontally simply by adding more DataNodes. NameNode allows incremental scaling of storage.

	- ADVANTAGES
		- Scalable across machines - storage capacity grows linearly by adding nodes.
		- Fault tolerant - file replication provides resilience against failure.
		- Designed for high throughput of large files rather than low latency.

Hadoop Commands:
	To copy from source file to destination file,
		hdfs dfs -cp source_file dest_file

	To overwite destination file, if it already exists,
		hdfs dfs -cp -f source_file dest_file

	To copy source directory to destination directory,
		hdfs dfs -cp -r source_dir dest_dir

	To overwite destination directory, if it already exits,
		hdfs dfs -cp -f -r source_dir dest_dir

Hadoop Node types
	1) NameNode
		- It is main node of HDFS; also called master node.
		- It stores meta data in RAM for quick access and track the files across hadoop cluster.
		- If namenode fails, entire HDFC is inaccessible.
		- Namenode tracks all information from files such as
			- which file saved in cluster,
			- access time of file,
			- which user access a file on current time
	2) Secondary Node
		- It helps the primary namenode, and merges the namespaces.
		- It stores data when primary namenode fails, and is used to restart the primary namenode.
		- It requires huge memory for data storing
		- It runs on different machines for memory management;
		- Also, it is checking point of namenode
	3) DataNode
		- It stores actual data of HDFS; also called slave node.
		- No data is affected, even if the data node is failed.
		- As it stores actual data, it is configured with lot of disk space
		- It perform read and write operations as per client request.
		- And, Performance of DataNode are based on NameNode Instuctions.
	4) Checkpoint Node
		- It is designed to address the namenode drawbacks.
		- It tracks the latest checkpoint directory that has same structure.
		- It creates the checking point for NameNode namespace and downloads the edits and fsimage from NameNode and mering locally.
		- The new image are uploaded to NameNode. After this uploads the result to NameNode.
	5) Backup Node
		- It provide the functions to Check Point but it interact with NameNode and it supports the online streaming of filesystems.
		- In Backup Nodes namespace are available on main memory because it interact with primary node.
		- It maintains up-to date file namespace for streaming process.
		- Backup node having own memory so just save and copy the namespace from main memory.
	6) Job Tracker Node
		- Job Tracker node used to MapReduce jobs and it process runs on separate node.
		- It receives the request for MapReduce from client.
		- It Loacte the data when after talking of NameNode.
		- Job tracked choose best TaskTracker for executes the task and it give slots to tasktracker for execute the task.
		- It monitors the all TaskTrackers and give status report to Client.
		- If Job Tracked failure MapReduce function does not executed and all functions are halted so Job Tracker is critical for MapReduce.
	7) Task Tracker Node
		- Task Tracker are runs on DataNode.
		- TaskTrackers will be assigned Mapper and Reducer tasks to execute by JobTracker.
		- If Task Tracker failure the job tracker assign the task for another node so MapReduce Task running successfully..


namenode vs datanode
 - NameNode is the master node in HDFS that manages the file system metadata while
   DataNode is a slave node in HDFS that stores the actual data as instructed by the NameNode.

Datanodes
	- DataNodes are responsible for serving read and write requests from the file system's clients.
	- DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode.

master vs datanode
---------------
	- Data nodes store the data, and participate in the cluster's indexing and search capabilities, while
	- master nodes are responsible for managing the cluster's activities and storing the cluster state, including the metadata.

jobtracker vs tasktracker
---------------
	- JobTracker is the service within Hadoop that is responsible for taking client requests.
	- It assigns them to TaskTrackers on DataNodes where the data required is locally present.
	- If that is not possible, JobTracker tries to assign the tasks to TaskTrackers within the
	  same rack where the data is locally present.

HDFS vs NFS
---------------
	- HDFS is a file system where data is distributed among many data nodes
	  so, more reliability, when if one of those systems, was failed.
	- NFS is file system or protocol which allows its clients to access file
	  over the network. All data resides on a single machine. Not much reliable as HDFS.

bucket vs HDFS
---------------
	Platform:
		Buckets are a storage service provided by Google Cloud Platform (GCP), while HDFS is a file system designed for the Apache Hadoop platform.
	Architecture:
		Buckets are a single-tenant storage system, meaning that each bucket is owned and managed by a single GCP project or user. HDFS is a distributed file system designed to be used in a cluster of computers.
	Data Replication:
		Buckets automatically replicate data across multiple data centers for redundancy and high availability, while HDFS stores data across multiple nodes in a cluster.
	APIs:
		Buckets can be accessed using APIs such as REST APIs, Google Cloud SDK, or client libraries, while HDFS has its own APIs and protocols for accessing data.
	Use Cases:
		Buckets are designed for storing unstructured data such as images, videos, logs, and other types of files, while HDFS is designed for storing and processing large data sets using distributed computing.

	Scalability:
		Both Buckets and HDFS are designed to be highly scalable and able to handle large amounts of data. However, HDFS is typically used in large-scale distributed computing environments, which may make it more suitable for extremely large datasets.
	Availability:
		Buckets automatically replicate data across multiple data centers for redundancy and high availability, which can make it more reliable than HDFS, which requires manual configuration for high availability.
	Cost:
		The cost of using Buckets versus HDFS depends on the specific cloud provider and usage patterns. Generally, Buckets are priced based on the amount of data stored and data egress, while HDFS requires dedicated hardware and may have higher upfront costs.
	Integration:
		Buckets are part of the Google Cloud Platform ecosystem and integrate with other GCP services such as BigQuery and Dataflow, which can make it easier to build and deploy data pipelines. HDFS, on the other hand, is typically used with Apache Hadoop and other open-source big data tools.

Machine learning Models
========================
1) Supervised Learning
	a) Regression
		- Linear Regression
			- Multiple Linear
			- Polynomial
		- Decision Tree
		- Random Forest
		- Neural Network

	b) Classification
		- Logistic Regression
		- Support Vector Machine
		- Naive Bayes
		- Decision Tree, Random Forest, Neural Network

2) Un-supervised Learning -> Patterns from input data without references to labeled outcomes
	- Clustering ->
		- K-means
		- Hierarchical
		- Mean shift
		- Density based
	- Dimensionality Reduction - process of reducing the dimension of your feature set
		- Feature elimination
		- Feature Extraction
			- Principal Component Analysis


Precision
	indicates the positive prediction made by the model
	It refers to the number of true positives divided by total number of postive predictions

Accurancy Vs precision
	Accurancy tells how many times the ML model was correct overall
	Precision is how good the model is at predicting a sepcific category

- High Performance Computing, in python
---------------------------------------
	- Dask
		- for distributed computing.
		- facilitates running many computations at the same time, either on a single machine or on many separate computers (cluster).
		- Under the hood, Dask breaks a single large data processing job into many smaller tasks, which are then handled by numpy or pandas
		- https://dask.org/
	- Modin
		- designed to parallelize pandas DataFrames by automatically distributing the
		  computation across all of the system’s available CPU cores.
		- Modin simply divides an existing DataFrame into different parts such that each part can be sent to a different CPU core. And to be more precise, Modin partitions the DataFrames across both rows and columns, which makes its parallel processing highly scalable for DataFrames of any size and shape.
		- https://github.com/modin-project/modin
	- vaex — 7k GitHub stars
	- datatable — 1.5k GitHub stars
	- cuDF — ~4.5k GitHub stars
	- pyspark
	- Koalas — ~3k GitHub stars
	- polars — ~5k GitHub stars

=======================================================================
Q1) Describe truthy vs falsy
Ans) Python supports boolean type: True, False

	bool() -- builtin function which will result the boolean-ness of the object in it.

	For integers,
		0 			-> False
		non-zero 	-> True
	For floats,
		0.0			-> False
		non-zero	-> True
		0.000001    -> True

	For None		-> False

	For boolean
		False		-> False
		True 		-> True

	For empty collections,
		empty list 	[] 		-> False
		empty tuple () 		-> False
		empty set   set()	-> False
		empty dict 	{}		-> False

		with atleast one element, it is True

	For relational and logical operations, based on result
		logical and --> will result in True, if all are True ONLY, else False
		logical OR  --> will result in False, if all are False ONLY, else True

Q2) How do you get the last item of a list?
Ans) we can use the reverse index to retrieve the last element from the list

	 Ex: mylist = [23, 45,  6,  3, 23, 65,  2,   4]
				   0   1    2   3   4   5   6     7  <----- forward indexing
				   -8  -7  -6  -5  -4  -3  -2    -1  <------ reverse indexing

		lastnum = mylist[-1]

		Also, we can get indirectly (not preferred) by
			lastNum = mylist[len(mylist) - 1]

Q3) What does a leading _ (underscore) before a variable typically identify in python (used as a common naming convention)?
Ans) Python follows naming convention called name mangling for the variables/methods

		name 	-> Public variable  	- no leading underscores - can be accessed directly
		_name	-> Protected variables  - one leading underscore - can be accessed directly
		__name	-> Private variables 	- two leading underscores - prepend class name as protected variable before it to access
											Ex: _ClassName__name

		__name__ -> built-in variables, or dunder methods, or magic methods
					there are some specific methods, as such.

What is a magic method ( aka dunder method)?
Ans) These are the builtin methods with some specific usage
	As they start and end with two (double) underscores, they were called as dunder (double-underscore) methods.
	Examples:
		__init__		constructor method - responsible for added default actions, after creating instance
		__del__			destructor method  - will invoke when deleting the instance
		__new__			method responsible for creating the instance, from a class

		__slot__				will restrict the attributes of class instance

		__enter__ & __exit__ 	used for creating custom context managers

		__iter__ & __next__ 	used for creating custom iterator classes

	Also, if we create the class methods with dundermethods like __add__, then
	with the instances, instead of addressing with this method names like

		instance1.__add__(instance2)

	we can also do as below
		instance1 + instance2


Q4) What was introduced in python 3.6 that changed string formatting?
What do you prefer: f-string or .format()?
Ans) String formattings in python
		- old style formatting  - using % with the specific data types
			Ex: language = "%s is rank %d language " %("python", 1)


		- new style formatting - using format() with placeholders, and not specific on data types
			Ex: language = "{0} is rank {1} language ".format("python", 1)

		- F-strings (from python 3.6)  -- advanced formatting
			Ex:
				lang = "python"
				rank = 1
				language = f"{lang} is rank {rank} language "

			Also, we can do computations within the braces to do more operations

				language = f"{lang.upper()} is rank {float(rank)} language "

		- F-string assignments ( from python 3.8)
			it will help in the debugging results.
			Instead of
				print("langauge = ", language)
				print("langauge = %s"% language)
				print("langauge = {}".format(language))
				print(f"langauge = {language}")

			we can write as
				print(f"{language =}"

		Finally, among the three (old-style, new-style with format() and f-strings), the
		F-string based formatting with take the least amount of time.

		So, i prefer F-strings among all.

Q5) How to cache data in Python
Ans) It can be done in 3 ways
		 1) Using cache, like
				- functool.lru_cache
				- Memcache
				- or, redis cache
		 2) Using sessions  if is a web application
		 3) Using the database, for persistence

		Also,
			We can import in the __init__.py file in package level and import
			It will be there for the entire session

1) Difference between JSON and XML data in REST API
Ans) Worked mostly with JSON for REST APIs
		REST APIs support both.
		JSON object has a type, whereas XML data is typeless

3) Difference between Git Add and Git Reset commands
Ans) git add command will stage the give file/folders, for next commit.
	 git reset command will undo the uncommited-yet-staged files/folders
		In addition, to undo file changes too, we can use
			git reset --hard


5)Why use Django when compared with flask framework?
Ans)flask is suitable when no-sql database is of choice, or when small utility application is needed, or when we need quick development
	Django is suitable when full-fledged application is needed, or when production-ready application is needed,

6) Difference between _init() and __str_() methods ?
Ans) __init__() is constructor method
	 __str__() is dunder method which is called which print() function is used to display class instance


Python Programming
==================
1) Give some examples of data types in python?
Ans)Basic Data types 			- Int, Float, String, None, Bool
	Built-in Data Structures 	- List, Tuple, Set, Dictionary

2) What is the difference between a list and a tuple?
Ans)List is mutable object, means we can edit the object like append, delete, ..
	where as tuple is immutable
	Tuples are faster than lists
		- Tuples are stored in a single block of memory. Tuples are immutable so,
		It doesn't require extra space to store new objects.
		- Lists are allocated in two blocks:
			1. fixed one with all the Python object information, and
			2. variable sized block for the data.
	- USAGE:
		List is used when we need to make changes in code, say adding values while loop .
		Tuple is used when we wont change something in the code

3) When is it a good idea to use a dictionary
Ans)Dictionary is good when there are more reads than writes.
	For reading a key in dict, it is O(1) as keys are unique (hashed and stored).

4) What is a lambda function and why would you use it
Ans)It is a one liner function, mainly useful for evaluating a single expression that is supposed to be evaluated only once.
	- SYNTAX is lambda arguments: expression
	- We need not define a complete function. We can use it instantly for the purpose.
	- It can be passed as a parameter to a higher-order function, like filter(),
	map(), and reduce().
	- example
		- if we have to increment all values in a list by one,
				map(lambda x:x + 1, range(10))
	- And, coming to the con side,
		- It can’t perform multiple expressions.
		- It can easily become cumbersome, for example when it includes an if-elif-else cycle.
		- It can’t contain any variable assignments (e.g., lambda x: x=0 will throw a SyntaxError).
		- We can’t provide a docstring to a lambda function.

5) How to get the last character in a string
Ans) Using the negative index, we can get.
	Ex:word = "Hello"
		last_char = word[-1]

6) What structure do you use to handle an exception (bonus: when does an else block get executed)?
Ans)In Python, exception handling has FOUR blocks. Else and Finally blocks were optional.
	1) Try --- executed every time
	2) Except--- executed when there is exception in try block, check if it can handle break code if it cant
	3) Else--- executed when no exception in try block
	4) Finally-- executed every time— irrespective of failure, it is like for file closing, etc ….

7) What is the difference between a class and an object?
Ans)The instances of class are called objects.
	Class defines the properties and behavior of its instances.
	We can create instances from all classes, except the abstract classes.

8) How would you break apart a comma-delimited list, then put it back together?
Ans) Hoping the question is comma-delimited string. if so,
	Ex: mystr = "a,b,c,d,1,2"
		chars = mystr.split(',') # splits the string to list of chars
		joined = ','.join(chars) # joining the list of strings, to a single string, with again comma as the delimiter

9) How to denote a comment in python?
Ans) Unlike many languages (like java, c++, ...), python dont have multiple comments, as it is an interpreter based language, where only one line of code is processed by the interpreter each time. Python has only line comments, with # (hash/pound) operator. It will have its effect from the place it is defined, till the end of that line. This operator will be treated as ordinary character, if it is part of any string.

10) When would you use the keyword self?
Ans)self acts as a placeholder of the instance, being passed to the class.
	It is not a python keyword, but PEP 8 recommends using it.

11) previous month last date
Ans)
	from datetime import date, timedelta

	curr_month_first_date = date.today().replace(day=1)
	last_month_last_date = curr_month_first_date - timedelta(days=1)
	print(last_month_last_date)


1) What is your experience with building Restful APIs using Python/Flask Framework?
Ans) Yes, I have experience in creating web applications and REST APIs using python frameworks like Flask, Django & FastAPI.
	Worked on both MVC and MVT architectures.
	Worked with sqlachemy for ORM, with flask applications
	Worked on Albemic for sql migrations, with flask applications

2) Have you worked with any testing frameworks in Python, such as unittest, pytest or behave? which one do you prefer and why ?
Ans) Worked majorly with Test Driven Development, with python test frameworks - unittest and pytest.
	When working with 3rd party REST APIs or Database connections, used mocking in unittest and pytest fixtures.
	Also, Worked with BDD - Behaviour Driven Development with behave framework. It is good in scenarios when test script need to be understandable by non-developers.
	I prefer, Pytest to unittest, because of lesser boilerplate code.

3) Have you worked on testing APIs/microservices with Python?
Ans) Yes, I worked on testing APIs and microservices with Python. It can be verifying response codes, serialization, input validation, authentication, etc. Common approaches include writing test clients using requests library, or integrated frameworks like pytest-flask. Automated integration/contract testing were also used.

4) Have you worked on regression test automation of reports (excel/csv based) ?
Ans) yes, I worked on Locust python framework and Jmeter scripts for load/regression testing. Also, worked with Robotframework too. Used Python modules (csv, pandas, ..) for parsing/creating/transfoming the excel/csv reports/datasets.

5) How do you approach test automation design to ensure maintainability, scalability and reusability of test code?
Ans) In my view, a good test automation follows principles like DRY, separation of concerns, and atomic/modular tests. Abstraction using pages, components and utility functions improves maintainability. CI/CD pipeline integration and version control also enable collaboration and change management.

Networking( only answer this if you have experience in this area)
======================================================================
1) Give me an example of a layer 4 protocol
Ans) In the OSI model, layer 4 protocol example include
		UDP - User Datagram Protocol
		TCP - Transmission Control Protocol

2) Give me an example of a layer 7 protocol
Ans)Layer 7 protocol examples include
		HTTP - HyperText Transfer Protocol - for internet communication
		SMTP - Simple Mail Transfer Protocol - for email communication

3) There are two parts of an HTTP request the headers and body, what are some examples of headers found in an HTTP request
Ans) A typical HTTP request header contains,
		HTTP Method, URL and HTTP version
		Host 		- server FQDN name
		User-Agent 	- Source (browser or program based) of the request
		Accept 		- What types of data is accepted
		Accept-Language - language of communication
		Accept-Encoding - Types of encoding supported
		Connection	 - connection type - keep-alive or closed
		Cache-Control- If cache present, the age of cache. If default, max-age=0

4) What is the difference between HTTP and HTTPS?
Ans)HTTP with encryption is called HTTPS.
	HTTPS uses TLS (earlier SSL) to encrypt normal HTTP requests and responses.
	So, HTTPS is more secure than HTTP.

5) What tool can be used to snoop on network traffic reaching the local machine
Ans) WireShark, TCPDump and WinDump are the most commonly used sniffing tools.
	I worked with wireshark, which gives the .pcap files. I analyzed them programmatically to pull insights.

6) Describe the steps of a TLS handshake (advanced)
Ans) TLS came as a replacement for SSL.
	SSL - Secure Sockets Layer
	TLS - Transport Layer Security

	During the TLS handshake, the client and server together will decide on the TLS version, the cipher which suits both.
	Authenticate the identity of the server via the server's public key and the SSL certificate authority's digital signature.
	Generate session keys in order to use symmetric encryption after the handshake is complete

	The RSA key exchange algorithm is the most common way for the TLS handshake.
	In RSA Key exchange,
		1) Client initiates handshake by sending a hello message, containing
			TLS version the client supports
			cipher suites supported
			string of random bytes, called client random
		2) Server replies with message containing
			the server's SSL certificate,
			the server's chosen cipher suite,
			another string of random bytes, called server random
		3) Client verifies the server's SSL certificate with certificate authority that issued it.
		4) Once verified, the client sends one more random string of bytes, called premaster secret.
		 This premaster secret is encrypted with a public key(which it got from SSL certificate)
		 and can only be decrypted with a private key by the server.
		5) The server decrypts the premaster secret with its private key
		6) Both client and server generate session keys from the client random, server random and the premaster secret.
		 Both results should be the same.
		7) Client sends a "finished" message that is encrypted with a session key.
		8) Server sends a "finished" message encrypted with a session key.
		9) The handshake is completed, and communication continues using the session keys.

7) Difference between subnet and subnet mask
Ans) subnet is part of larger network which is subdivided into smaller network segments, each with its own unique network address.
	Subnet mask is used to determine which bits of the IP address identifies network part and which bits represent host part.
	subnet mask is a 32 bit value, which we apply on any IP address.

Communication/Process
======================
1) Describe your understanding of the scrum process
Ans) It is a project management system. There are two types:
	a) Agile scrum methodology works on incremental development.
			There will be sprint planning, where we plan what to do in the next sprint
			We have a sprint for 2 weeks, in general.
			We have daily standup calls, where we discuss the progress of these jiras, and any blockers.
			At the end of the sprint, we have sprint reviews and retrospective calls.
	b) Kanban
			It is suitable for bug management , mainly.
			Each ticket has a specific weightage and it will go as per this SLA.

2) What is your communication style when chatting on slack
Ans) I will try to explain the point, to avoid ambiguity. For Example, If i am struggling with a problem
	a) I will try to precisely explain the problem
	b) Then, I will list out the different ways of addressing the problem, and corresponding results/failures.
	c) If i have any thoughts, i will ask to suggest among them.

	When someone replies, we will proceed, point-to-point reply

To get only duplicate numbers, in a list
	list1 = [2, 1, 4, 2, 1, 2]
	uniques = set()
	dups = []
	for num in list1:
		if num in uniques:
			dups.append(num)
		else:
			uniques.add(num)

	print(dups)

highest - lowest, rearranging a number

	num = 213
	integers = list(str(num))
	highest = int(''.join(sorted(integers, reverse=True)))
	smallest = int("".join(sorted(integers)))
	print(highest - smallest)

TODO
	ML - https://alphasignal.ai/
